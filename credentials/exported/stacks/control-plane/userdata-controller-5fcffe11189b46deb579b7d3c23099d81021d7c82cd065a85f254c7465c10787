#cloud-config
coreos:
  update:
    reboot-strategy: "off"
  flannel:
    interface: $private_ipv4
    etcd_cafile: /etc/kubernetes/ssl/etcd-trusted-ca.pem
    etcd_certfile: /etc/kubernetes/ssl/etcd-client.pem
    etcd_keyfile: /etc/kubernetes/ssl/etcd-client-key.pem

  units:
    - name: docker.service
      drop-ins:
        - name: 10-fast-restart.conf
          content: |
            [Service]
            Restart=always
            RestartSec=1s
    - name: telegraf.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Teleport Monitoring Agent
        After=network-online.target
        After=etcd-member.service
        [Service]
        Restart=on-failure
        ExecStart=/bin/rkt run \
          --stage1-name=coreos.com/rkt/stage1-fly:1.26.0 \
          --set-env TELEGRAF_GLOBAL_TAGS_CLUSTER=production \
          --set-env TELEGRAF_GLOBAL_TAGS_SERVICE=controller \
          --set-env HOST_ETC=/etc2 \
          --interactive \
          --volume=varlibetcd2,kind=host,source=/var/lib/etcd2 \
          --volume=etc,kind=host,source=/etc \
          --mount volume=etc,target=/etc2 \
          --mount volume=varlibetcd2,target=/var/lib/etcd2 \
          --hostname=%H \
          --dns=host \
          --net=host \
          --insecure-options=image,ondisk \
          docker://checkr/checkr-telegraf-kapacitor:latest \
          --exec /usr/bin/telegraf -- --config /etc/generic.system.telegraf.conf
    - name: docker-healthcheck.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Run docker-healthcheck once
        After=docker.service
        
        [Service]
        Type=oneshot
        ExecStart=/opt/bin/docker-healthcheck
        
        [Install]
        WantedBy=multi-user.target
    - name: docker-healthcheck.timer
      command: start
      enable: true
      content: |
        [Unit]
        Description=Trigger docker-healthcheck periodically
        After=docker.service
        
        [Timer]
        OnUnitInactiveSec=30s
        Unit=docker-healthcheck.service
        
        [Install]
        WantedBy=multi-user.target


    - name: cfn-etcd-environment.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Fetches etcd static IP addresses list from CF
        After=network-online.target

        [Service]
        Restart=on-failure
        RemainAfterExit=true
        ExecStartPre=/opt/bin/cfn-etcd-environment
        ExecStart=/usr/bin/mv -f /var/run/coreos/etcd-environment /etc/etcd-environment


    - name: docker.service
      drop-ins:

        - name: 10-post-start-check.conf
          content: |
            [Service]
            RestartSec=10
            ExecStartPost=/usr/bin/docker pull gcr.io/google_containers/pause-amd64:3.0

        - name: 40-flannel.conf
          content: |
            [Unit]
            Wants=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
            ExecStartPre=/usr/bin/systemctl is-active flanneld.service

        - name: 60-logfilelimit.conf
          content: |
            [Service]
            Environment="DOCKER_OPTS=--log-opt max-size=50m --log-opt max-file=3"

    - name: flanneld.service
      drop-ins:
        - name: 10-etcd.conf
          content: |
            [Unit]
            Wants=cfn-etcd-environment.service
            After=cfn-etcd-environment.service

            [Service]
            EnvironmentFile=-/etc/etcd-environment
            Environment="ETCD_SSL_DIR=/etc/kubernetes/ssl"
            EnvironmentFile=-/run/flannel/etcd-endpoints.opts
            ExecStartPre=/usr/bin/systemctl is-active cfn-etcd-environment.service
            ExecStartPre=/bin/sh -ec "echo FLANNELD_ETCD_ENDPOINTS=${ETCD_ENDPOINTS} >/run/flannel/etcd-endpoints.opts"
            ExecStartPre=/opt/bin/decrypt-assets
            ExecStartPre=/usr/bin/etcdctl \
            --ca-file=/etc/kubernetes/ssl/etcd-trusted-ca.pem \
            --cert-file=/etc/kubernetes/ssl/etcd-client.pem \
            --key-file=/etc/kubernetes/ssl/etcd-client-key.pem \
            --endpoints="${ETCD_ENDPOINTS}" \
            set /coreos.com/network/config '{"Network" : "10.2.0.0/16", "Backend" : {"Type" : "vxlan"}}'
            TimeoutStartSec=120


    - name: kubelet.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=flanneld.service cfn-etcd-environment.service
        After=cfn-etcd-environment.service
        [Service]
        # EnvironmentFile=/etc/environment allows the reading of COREOS_PRIVATE_IPV4
        EnvironmentFile=/etc/environment
        EnvironmentFile=-/etc/etcd-environment
        Environment=KUBELET_IMAGE_TAG=v1.7.6_coreos.0
        Environment=KUBELET_IMAGE_URL=quay.io/coreos/hyperkube
        Environment="RKT_RUN_ARGS=--volume dns,kind=host,source=/etc/resolv.conf \
        --set-env=ETCD_CA_CERT_FILE=/etc/kubernetes/ssl/etcd-trusted-ca.pem \
        --set-env=ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd-client.pem \
        --set-env=ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd-client-key.pem \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume var-lib-cni,kind=host,source=/var/lib/cni \
        --mount volume=var-lib-cni,target=/var/lib/cni \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        --volume cni-bin,kind=host,source=/opt/cni/bin \
        --mount volume=cni-bin,target=/opt/cni/bin \
        --volume etc-kubernetes,kind=host,source=/etc/kubernetes \
        --mount volume=etc-kubernetes,target=/etc/kubernetes"
        ExecStartPre=/usr/bin/systemctl is-active flanneld.service
        ExecStartPre=/usr/bin/systemctl is-active cfn-etcd-environment.service
        ExecStartPre=/usr/bin/mkdir -p /var/lib/cni
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/usr/bin/etcdctl \
                       --ca-file /etc/kubernetes/ssl/etcd-trusted-ca.pem \
                       --key-file /etc/kubernetes/ssl/etcd-client-key.pem \
                       --cert-file /etc/kubernetes/ssl/etcd-client.pem \
                       --endpoints "${ETCD_ENDPOINTS}" \
                       cluster-health

        ExecStartPre=/bin/sh -ec "find /etc/kubernetes/manifests /srv/kubernetes/manifests  -maxdepth 1 -type f | xargs --no-run-if-empty sed -i 's|#ETCD_ENDPOINTS#|${ETCD_ENDPOINTS}|'"
        ExecStartPre=/bin/sh -ec "find /etc/kubernetes/cni/net.d/ -maxdepth 1 -type f | xargs --no-run-if-empty sed -i 's|#ETCD_ENDPOINTS#|${ETCD_ENDPOINTS}|'"
        ExecStartPre=/usr/bin/docker run --rm -e SLEEP=false -e KUBERNETES_SERVICE_HOST= -e KUBERNETES_SERVICE_PORT= -v /opt/cni/bin:/host/opt/cni/bin quay.io/calico/cni:v1.10.0 /install-cni.sh
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --kubeconfig=/etc/kubernetes/controller-kubeconfig.yaml \
        --require-kubeconfig \
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
        --cni-bin-dir=/opt/cni/bin \
        --network-plugin=cni \
        --container-runtime=docker \
        --rkt-path=/usr/bin/rkt \
        --rkt-stage1-image=coreos.com/rkt/stage1-coreos \
        --node-labels node-role.kubernetes.io/master \
        --register-with-taints=node.alpha.kubernetes.io/role=master:NoSchedule \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --cluster-dns=10.3.0.10 \
        --cluster-domain=cluster.local \
        --cloud-provider=aws \
        $KUBELET_OPTS
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target



    - name: install-kube-system.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=kubelet.service docker.service

        [Service]
        Type=oneshot
        StartLimitInterval=0
        RemainAfterExit=true
        ExecStartPre=/usr/bin/bash -c "until /usr/bin/systemctl is-active kubelet.service; do echo waiting until kubelet starts; sleep 10; done"
        ExecStartPre=/usr/bin/bash -c "until /usr/bin/systemctl is-active docker.service; do echo waiting until docker starts; sleep 10; done"
        ExecStartPre=/usr/bin/bash -c "until /usr/bin/curl -s -f http://127.0.0.1:8080/version; do echo waiting until apiserver starts; sleep 10; done"
        ExecStart=/opt/bin/retry 3 /opt/bin/install-kube-system

    - name: apply-kube-aws-plugins.service
      command: start
      runtime: true
      content: |
        [Unit]
        Requires=install-kube-system.service
        After=install-kube-system.service

        [Service]
        Type=oneshot
        StartLimitInterval=0
        RemainAfterExit=true
        ExecStart=/opt/bin/retry 3 /opt/bin/apply-kube-aws-plugins



    - name: cfn-signal.service
      command: start
      content: |
        [Unit]
        Wants=kubelet.service docker.service install-kube-system.service apply-kube-aws-plugins.service
        After=kubelet.service install-kube-system.service apply-kube-aws-plugins.service

        [Service]
        Type=simple
        Restart=on-failure
        RestartSec=60
        StartLimitInterval=640
        StartLimitBurst=10
        ExecStartPre=/usr/bin/systemctl is-active install-kube-system.service
        ExecStartPre=/usr/bin/systemctl is-active apply-kube-aws-plugins.service
        ExecStartPre=/usr/bin/bash -c "while sleep 1; do if /usr/bin/curl -s -m 20 -f  http://127.0.0.1:8080/healthz > /dev/null &&  /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10252/healthz > /dev/null && /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10251/healthz > /dev/null &&  /usr/bin/curl --insecure -s -m 20 -f  https://127.0.0.1:10250/healthz > /dev/null ; then break ; fi;  done"
        
        ExecStartPre=/usr/bin/bash -c "until /usr/bin/docker run --net=host --pid=host --rm calico/ctl:v1.5.0 node status > /dev/null; do sleep 3; done && echo Calico running"
        
        ExecStart=/opt/bin/cfn-signal









write_files:
  - path: /etc/rkt/auth.d/docker.json
    permissions: 0600
    encoding: gzip+base64
    content: H4sIAAAAAAAA/6pWUCrKLvHOzEtRslJQSslPzk4tciwtyVDSAUuEpRYVZ+bngeTKDMFiqemZxSVFmanFSlYK0TBupa6hHkSvXma+UqyOglJyUWpKal5JZmIOSGG1glJpcWoRyJjk/JTUtJz88oKi/BSQgQWJxcXl+UVg60NLjaKcSqOS3EJDjDN8080CTAtylBRqa7kAAQAA//+faTbrpgAAAA==
  - path: /opt/bin/docker-healthcheck
    permissions: 0700
    encoding: gzip+base64
    content: H4sIAAAAAAAA/7yRMW7rMBBEe55ivv4BpMCAGwNpc4TUFDmiFqFJY7lWktsHEiTETYo0aQeLh/fI///6UUo/+jY7JxNMrqx3w9OAWMMbFbeGZ/SRS1/uOV9gM4vjhxgGN4lzDHNFtx9PXjJjt48vskhJB+g0oDHUEhuS+kCMnKoSymZeTUrqXMvkDafhVy6PAspQF+rm8INjM8kZsb6XC0wlJeqD5W7TufbZjNdg+ZgQajEvhRr34wP86mXVx/m70CqSLDyoa8q6Hewt8/wHmeuHbC/7FQAA//+efA6i6QEAAA==


  - path: /etc/kubernetes/additional-configs/cloud.config
    owner: root:root
    permissions: 0644
    content: |
      [global]
      DisableSecurityGroupIngress = true


  - path: /opt/bin/apply-kube-aws-plugins
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -vxe

      kubectl() {
          /usr/bin/docker run --rm --net=host \
            -v /etc/resolv.conf:/etc/resolv.conf \
            -v /srv/kube-aws/plugins:/srv/kube-aws/plugins \
            quay.io/coreos/hyperkube:v1.7.6_coreos.0 /hyperkube kubectl "$@"
      }

      helm() {
          /usr/bin/docker run --rm --net=host \
            -v /etc/resolv.conf:/etc/resolv.conf \
            -v /srv/kube-aws/plugins:/srv/kube-aws/plugins \
            quay.io/kube-aws/helm:v2.6.0 helm "$@"
      }

      while read m || [[ -n $m ]]; do
        kubectl apply -f $m
      done </srv/kube-aws/plugins/kubernetes-manifests

      while read r || [[ -n $r ]]; do
        release_name=$(jq .name $r)
        chart_name=$(jq .chart.name $r)
        chart_version=$(jq .chart.version $r)
        values_file=$(jq .values.file $r)
        if helm status $release_name; then
          helm upgrade $release_name $chart_name --version $chart_version -f $values_file
        else
          helm install $release_name $chart_name --version $chart_version -f $values_file
        fi
      done </srv/kube-aws/plugins/helm-releases



  - path: /opt/bin/cfn-signal
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false \
        --mount volume=awsenv,target=/var/run/coreos \
        --uuid-file-save=/var/run/coreos/cfn-signal.uuid \
        --net=host \
        --trust-keys-from-https \
        quay.io/coreos/awscli:master --exec=/bin/bash -- \
          -ec \
          'instance_id=$(curl http://169.254.169.254/latest/meta-data/instance-id)
           stack_name=$(
             aws ec2 describe-tags --region us-east-1 --filters \
               "Name=resource-id,Values=$instance_id" \
               "Name=key,Values=aws:cloudformation:stack-name" \
               --output json \
             | jq -r ".Tags[].Value"
           )
           cfn-signal -e 0 --region us-east-1 --resource Controllers --stack $stack_name
          '

      rkt rm --uuid-file=/var/run/coreos/cfn-signal.uuid || :

  - path: /opt/bin/cfn-etcd-environment
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false \
        --mount volume=awsenv,target=/var/run/coreos \
        --uuid-file-save=/var/run/coreos/cfn-etcd-environment.uuid \
        --net=host \
        --trust-keys-from-https \
        quay.io/coreos/awscli:master --exec=/bin/bash -- \
          -ec \
          'instance_id=$(curl http://169.254.169.254/latest/meta-data/instance-id)
           stack_name=$(
             aws ec2 describe-tags --region us-east-1 --filters \
               "Name=resource-id,Values=$instance_id" \
               "Name=key,Values=aws:cloudformation:stack-name" \
               --output json \
             | jq -r ".Tags[].Value"
           )
           cfn-init -v -c "etcd-client" --region us-east-1 --resource Controllers --stack $stack_name
          '

      rkt rm --uuid-file=/var/run/coreos/cfn-etcd-environment.uuid || :

  - path: /etc/default/kubelet
    permissions: 0755
    owner: root:root
    content: |
      KUBELET_OPTS=""

  - path: /opt/bin/install-kube-system
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e

      kubectl() {
          /usr/bin/docker run --rm --net=host -v /srv/kubernetes:/srv/kubernetes quay.io/coreos/hyperkube:v1.7.6_coreos.0 /hyperkube kubectl "$@"
      }

      while ! kubectl get ns kube-system; do
        echo Waiting until kube-system created.
        sleep 3
      done

      mfdir=/srv/kubernetes/manifests

      
      /bin/bash /opt/bin/populate-tls-calico-etcd
      kubectl apply -f "${mfdir}/calico.yaml"
      

      

      # Configmaps
      kubectl apply -f "${mfdir}/kube-dns-cm.yaml"

      # Service Accounts
      for manifest in {kube-dns,heapster}; do
          kubectl apply -f "${mfdir}/$manifest-sa.yaml"
      done

      # Install tiller by default
      kubectl apply -f "${mfdir}/tiller.yaml"



      # Deployments
      for manifest in {kube-dns,kube-dns-autoscaler,kube-dashboard,heapster}; do
          kubectl apply -f "${mfdir}/$manifest-de.yaml"
      done

      # Services
      for manifest in {kube-dns,heapster,kube-dashboard}; do
          kubectl apply -f "${mfdir}/$manifest-svc.yaml"
      done

      mfdir=/srv/kubernetes/rbac

      # Cluster roles and bindings
      for manifest in {node-extensions,}; do
          kubectl apply -f "${mfdir}/cluster-roles/$manifest.yaml"
      done
      for manifest in {kube-admin,system-worker,node,node-proxier,node-extensions,heapster}; do
          kubectl apply -f "${mfdir}/cluster-role-bindings/$manifest.yaml"
      done

      # Roles and bindings
      for manifest in {pod-nanny,}; do
          kubectl apply -f "${mfdir}/roles/$manifest.yaml"
      done
      for manifest in {heapster-nanny,}; do
          kubectl apply -f "${mfdir}/role-bindings/$manifest.yaml"
      done

      

      
        mfdir=/srv/kubernetes/manifests
        kubectl apply -f "${mfdir}/kube2iam-rbac.yaml"
        kubectl apply -f "${mfdir}/kube2iam-ds.yaml";
      

  - path: /etc/kubernetes/cni/docker_opts_cni.env
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""

  - path: /opt/bin/host-rkt
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/sh
      # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
      # through this rkt wrapper. It essentially enters the host mount namespace
      # (which it is already in) only for the purpose of breaking out of the chroot
      # before calling rkt. It makes things like rkt gc work and avoids bind mounting
      # in certain rkt filesystem dependancies into the kubelet rootfs. This can
      # eventually be obviated when the write-api stuff gets upstream and rkt gc is
      # through the api-server. Related issue:
      # https://github.com/coreos/rkt/issues/2878
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "$@"


  - path: /srv/kubernetes/manifests/calico.yaml
    content: |
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: calico-config
        namespace: kube-system
      data:
        etcd_endpoints: "#ETCD_ENDPOINTS#"
        cni_network_config: |-
          {
              "name": "calico",
              "type": "flannel",
              "delegate": {
                  "type": "calico",
                  "etcd_endpoints": "__ETCD_ENDPOINTS__",
                  "etcd_key_file": "__ETCD_KEY_FILE__",
                  "etcd_cert_file": "__ETCD_CERT_FILE__",
                  "etcd_ca_cert_file": "__ETCD_CA_CERT_FILE__",
                  "log_level": "info",
                  "policy": {
                      "type": "k8s",
                      "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                      "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
                  },
                  "kubernetes": {
                      "kubeconfig": "__KUBECONFIG_FILEPATH__"
                  }
              }
          }

        etcd_ca: "/calico-secrets/etcd-ca"
        etcd_cert: "/calico-secrets/etcd-cert"
        etcd_key: "/calico-secrets/etcd-key"

      ---

      apiVersion: v1
      kind: Secret
      type: Opaque
      metadata:
        name: calico-etcd-secrets
        namespace: kube-system
      data:
        etcd-key: "$ETCDKEY"
        etcd-cert: "$ETCDCERT"
        etcd-ca: "$ETCDCA"

      ---

      kind: DaemonSet
      apiVersion: extensions/v1beta1
      metadata:
        name: calico-node
        namespace: kube-system
        labels:
          k8s-app: calico-node
      spec:
        selector:
          matchLabels:
            k8s-app: calico-node
        updateStrategy:
          type: RollingUpdate
        template:
          metadata:
            labels:
              k8s-app: calico-node
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            tolerations:
            - operator: Exists
              effect: NoSchedule
            - operator: Exists
              effect: NoExecute
            - operator: Exists
              key: CriticalAddonsOnly
            hostNetwork: true
            containers:
              - name: calico-node
                image: quay.io/calico/node:v2.5.1
                env:
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  - name: CALICO_NETWORKING_BACKEND
                    value: "none"
                  - name: CLUSTER_TYPE
                    value: "kubeaws,canal"
                  - name: CALICO_DISABLE_FILE_LOGGING
                    value: "true"
                  - name: NO_DEFAULT_POOLS
                    value: "true"
                  - name: ETCD_CA_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_ca
                  - name: ETCD_KEY_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_key
                  - name: ETCD_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_cert
                securityContext:
                  privileged: true
                volumeMounts:
                  - mountPath: /lib/modules
                    name: lib-modules
                    readOnly: true
                  - mountPath: /var/run/calico
                    name: var-run-calico
                    readOnly: false
                  - mountPath: /calico-secrets
                    name: etcd-certs
                  - mountPath: /etc/resolv.conf
                    name: dns
                    readOnly: true
            volumes:
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-run-calico
                hostPath:
                  path: /var/run/calico
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: cni-net-dir
                hostPath:
                  path: /etc/kubernetes/cni/net.d
              - name: etcd-certs
                secret:
                  secretName: calico-etcd-secrets
              - name: dns
                hostPath:
                  path: /etc/resolv.conf

      ---

      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: calico-policy-controller
        namespace: kube-system
        labels:
          k8s-app: calico-policy
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ''

      spec:
        replicas: 1
        template:
          metadata:
            name: calico-policy-controller
            namespace: kube-system
            labels:
              k8s-app: calico-policy
          spec:
            tolerations:
            - key: "node.alpha.kubernetes.io/role"
              operator: "Equal"
              value: "master"
              effect: "NoSchedule"
            - key: "CriticalAddonsOnly"
              operator: "Exists"
            hostNetwork: true
            containers:
              - name: calico-policy-controller
                image: quay.io/calico/kube-policy-controller:v0.7.0
                env:
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  - name: ETCD_CA_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_ca
                  - name: ETCD_KEY_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_key
                  - name: ETCD_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_cert
                  - name: K8S_API
                    value: "https://kubernetes.default:443"
                  - name: CONFIGURE_ETC_HOSTS
                    value: "true"
                volumeMounts:
                  - mountPath: /calico-secrets
                    name: etcd-certs
            volumes:
              - name: etcd-certs
                secret:
                  secretName: calico-etcd-secrets

  - path: /opt/bin/populate-tls-calico-etcd
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      etcd_ca=$(cat /etc/kubernetes/ssl/etcd-trusted-ca.pem | base64 | tr -d '\n')
      etcd_key=$(cat /etc/kubernetes/ssl/etcd-client-key.pem | base64 | tr -d '\n')
      etcd_cert=$(cat /etc/kubernetes/ssl/etcd-client.pem | base64 | tr -d '\n')

      sed -i -e "s#\$ETCDCA#$etcd_ca#g" /srv/kubernetes/manifests/calico.yaml
      sed -i -e "s#\$ETCDCERT#$etcd_cert#g" /srv/kubernetes/manifests/calico.yaml
      sed -i -e "s#\$ETCDKEY#$etcd_key#g" /srv/kubernetes/manifests/calico.yaml





  - path: /opt/bin/decrypt-assets
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=kube,kind=host,source=/etc/kubernetes,readOnly=false \
        --mount=volume=kube,target=/etc/kubernetes \
        --uuid-file-save=/var/run/coreos/decrypt-assets.uuid \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        --trust-keys-from-https \
        quay.io/coreos/awscli:master --exec=/bin/bash -- \
          -ec \
          'echo decrypting assets
           shopt -s nullglob
           for encKey in /etc/kubernetes/{ssl,}/*.enc; do
             echo decrypting $encKey
             f=$(mktemp $encKey.XXXXXXXX)
             /usr/bin/aws \
               --region us-east-1 kms decrypt \
               --ciphertext-blob fileb://$encKey \
               --output text \
               --query Plaintext \
             | base64 -d > $f
             mv -f $f ${encKey%.enc}
           done;

           echo done.'

      rkt rm --uuid-file=/var/run/coreos/decrypt-assets.uuid || :




  # TODO: remove the following binding once the TLS Bootstrapping feature is enabled by default, see:
  # https://github.com/kubernetes-incubator/kube-aws/pull/618#discussion_r115162048
  # https://kubernetes.io/docs/admin/authorization/rbac/#core-component-roles

  # Makes kube-worker user behave like a regular member of system:nodes group,
  # needed when TLS bootstrapping is disabled
  - path: /srv/kubernetes/rbac/cluster-role-bindings/node.yaml
    content: |
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: kube-aws:node
        subjects:
          - kind: User
            name: kube-worker
        roleRef:
          kind: ClusterRole
          name: system:node
          apiGroup: rbac.authorization.k8s.io

  # We need to give nodes a few extra permissions so that both the node
  # draining and node labeling with AWS metadata work as expected
  - path: /srv/kubernetes/rbac/cluster-roles/node-extensions.yaml
    content: |
        kind: ClusterRole
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
            name: kube-aws:node-extensions
        rules:
          - apiGroups: ["extensions"]
            resources:
            - daemonsets
            verbs:
            - get
          # Can be removed if node authorizer is enabled
          - apiGroups: [""]
            resources:
            - nodes
            verbs:
            - patch
            - update
          - apiGroups: ["extensions"]
            resources:
            - replicasets
            verbs:
            - get
          - apiGroups: ["batch"]
            resources:
            - jobs
            verbs:
            - get
          - apiGroups: [""]
            resources:
            - replicationcontrollers
            verbs:
            - get
          - apiGroups: [""]
            resources:
            - pods/eviction
            verbs:
            - create
          - nonResourceURLs: ["*"]
            verbs: ["*"]

  # Grants super-user permissions to the kube-admin user
  - path: /srv/kubernetes/rbac/cluster-role-bindings/kube-admin.yaml
    content: |
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: kube-aws:admin
        subjects:
          - kind: User
            name: kube-admin
        roleRef:
          kind: ClusterRole
          name: cluster-admin
          apiGroup: rbac.authorization.k8s.io

  # Allows both `kube-worker` user and members of the `system:nodes` group
  # to perform actions needed by the `kube-proxy` component. Once kube-proxy
  # is migrated to DaemonSets, we could set up a ServiceAccount for it and
  # associate this role to it instead.
  - path: /srv/kubernetes/rbac/cluster-role-bindings/node-proxier.yaml
    content: |
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: kube-aws:node-proxier
        subjects:
          - kind: User
            name: kube-worker
          - kind: Group
            name: system:nodes
        roleRef:
          kind: ClusterRole
          name: system:node-proxier
          apiGroup: rbac.authorization.k8s.io

  # Allows add-ons running with the default service account in kube-sytem to have super-user access
  - path: /srv/kubernetes/rbac/cluster-role-bindings/system-worker.yaml
    content: |
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: kube-aws:system-worker
        subjects:
          - kind: ServiceAccount
            namespace: kube-system
            name: default
        roleRef:
          kind: ClusterRole
          name: cluster-admin
          apiGroup: rbac.authorization.k8s.io

  # TODO: remove the following binding once the TLS Bootstrapping feature is enabled by default, see:
  # https://github.com/kubernetes-incubator/kube-aws/pull/618#discussion_r115162048
  # https://kubernetes.io/docs/admin/authorization/rbac/#core-component-roles

  # Associates the add-on role `kube-aws:node-extensions` to all nodes, so that
  # extra kube-aws features (like node draining) work as expected
  - path: /srv/kubernetes/rbac/cluster-role-bindings/node-extensions.yaml
    content: |
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: kube-aws:node-extensions
        subjects:
          - kind: User
            name: kube-worker
          - kind: Group
            name: system:nodes
        roleRef:
          kind: ClusterRole
          name: kube-aws:node-extensions
          apiGroup: rbac.authorization.k8s.io

  # Allow heapster access to the built in cluster role via its service account
  - path: /srv/kubernetes/rbac/cluster-role-bindings/heapster.yaml
    content: |
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: heapster
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:heapster
        subjects:
        - kind: ServiceAccount
          name: heapster
          namespace: kube-system

  # Heapster's pod_nanny monitors the heapster deployment & its pod(s), and scales
  # the resources of the deployment if necessary.
  - path: /srv/kubernetes/rbac/roles/pod-nanny.yaml
    content: |
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: Role
        metadata:
          name: system:pod-nanny
          namespace: kube-system
          labels:
            kubernetes.io/cluster-service: "true"
            addonmanager.kubernetes.io/mode: Reconcile
        rules:
        - apiGroups:
          - ""
          resources:
          - pods
          verbs:
          - get
        - apiGroups:
          - "extensions"
          resources:
          - deployments
          verbs:
          - get
          - update

  # Allow heapster nanny access to the pod nanny role via its service account (same pod as heapster)
  - path: /srv/kubernetes/rbac/role-bindings/heapster-nanny.yaml
    content: |
        kind: RoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: heapster-nanny
          namespace: kube-system
          labels:
            kubernetes.io/cluster-service: "true"
            addonmanager.kubernetes.io/mode: Reconcile
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: system:pod-nanny
        subjects:
        - kind: ServiceAccount
          name: heapster
          namespace: kube-system



  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-proxy
          namespace: kube-system
          labels:
            k8s-app: kube-proxy
          annotations:
            rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly

        spec:
          hostNetwork: true
          containers:
          - name: kube-proxy
            image: quay.io/coreos/hyperkube:v1.7.6_coreos.0
            command:
            - /hyperkube
            - proxy
            - --master=http://127.0.0.1:8080
            securityContext:
              privileged: true
            volumeMounts:
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
            - mountPath: /var/run/dbus
              name: dbus
              readOnly: false
          volumes:
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
          - hostPath:
              path: /var/run/dbus
            name: dbus

  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
        labels:
          k8s-app: kube-apiserver
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:v1.7.6_coreos.0
          command:
          - /hyperkube
          - apiserver
          - --apiserver-count=2
          - --bind-address=0.0.0.0
          - --etcd-servers=#ETCD_ENDPOINTS#
          - --etcd-cafile=/etc/kubernetes/ssl/etcd-trusted-ca.pem
          - --etcd-certfile=/etc/kubernetes/ssl/etcd-client.pem
          - --etcd-keyfile=/etc/kubernetes/ssl/etcd-client-key.pem
          - --allow-privileged=true
          - --service-cluster-ip-range=10.3.0.0/24
          - --secure-port=443
          
          - --storage-backend=etcd3
          
          - --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP
          
          
          - --audit-log-maxage=30
          - --audit-log-path=/dev/stdout
          - --audit-log-maxbackup=1
          
          - --authorization-mode=RBAC
          
          - --advertise-address=$private_ipv4
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,DenyEscalatingExec
          - --anonymous-auth=false
          - --cert-dir=/etc/kubernetes/ssl
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1/networkpolicies=true,batch/v2alpha1,rbac.authorization.k8s.io/v1beta1=true
          - --cloud-provider=aws
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          
          
          
          - mountPath: /var/log
            name: var-log
            readOnly: false
          
          
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        
        
        
        - hostPath:
            path: /var/log
          name: var-log
        
        

  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
        labels:
          k8s-app: kube-controller-manager
      spec:
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:v1.7.6_coreos.0
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --cloud-provider=aws
          
          - --node-monitor-grace-period=300s
          
          
          - --cloud-config=/etc/kubernetes/additional-configs/cloud.config
          
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          
          - mountPath: /etc/kubernetes/additional-configs
            name: additional-configs
            readOnly: true
          
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        hostNetwork: true
        volumes:
        
        - hostPath:
            path: /etc/kubernetes/additional-configs
          name: additional-configs
        
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host

  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
        labels:
          k8s-app: kube-scheduler
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:v1.7.6_coreos.0
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15

  - path: /srv/kubernetes/manifests/kube-dns-sa.yaml
    content: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            kubernetes.io/cluster-service: "true"

  - path: /srv/kubernetes/manifests/kube-dns-cm.yaml
    content: |
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: kube-dns
          namespace: kube-system

  - path: /srv/kubernetes/manifests/kube-dns-autoscaler-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: kube-dns-autoscaler
          namespace: kube-system
          labels:
            k8s-app: kube-dns-autoscaler
            kubernetes.io/cluster-service: "true"
        spec:
          template:
            metadata:
              labels:
                k8s-app: kube-dns-autoscaler
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              containers:
              - name: autoscaler
                image: gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.1.2
                resources:
                    requests:
                        cpu: "20m"
                        memory: "10Mi"
                command:
                  - /cluster-proportional-autoscaler
                  - --namespace=kube-system
                  - --configmap=kube-dns-autoscaler
                  - --target=Deployment/kube-dns
                  - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"min":2}}
                  - --logtostderr=true
                  - --v=2



  - path: /srv/kubernetes/manifests/kube-dns-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            k8s-app: kube-dns
            kubernetes.io/cluster-service: "true"
        spec:
          # replicas: not specified here:
          # 1. In order to make Addon Manager do not reconcile this replicas parameter.
          # 2. Default is 1.
          # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
          strategy:
            rollingUpdate:
              maxSurge: 10%
              maxUnavailable: 0
          selector:
            matchLabels:
              k8s-app: kube-dns
          template:
            metadata:
              labels:
                k8s-app: kube-dns
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              volumes:
              - name: kube-dns-config
                configMap:
                  name: kube-dns
                  optional: true
              tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              containers:
              - name: kubedns
                image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4
                resources:
                  limits:
                    memory: 170Mi
                  requests:
                    cpu: 100m
                    memory: 70Mi
                livenessProbe:
                  httpGet:
                    path: /healthcheck/kubedns
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 8081
                    scheme: HTTP
                  initialDelaySeconds: 3
                  timeoutSeconds: 5
                args:
                - --domain=cluster.local.
                - --dns-port=10053
                - --config-dir=/kube-dns-config
                # This should be set to v=2 only after the new image (cut from 1.5) has
                # been released, otherwise we will flood the logs.
                - --v=2
                env:
                - name: PROMETHEUS_PORT
                  value: "10055"
                ports:
                - containerPort: 10053
                  name: dns-local
                  protocol: UDP
                - containerPort: 10053
                  name: dns-tcp-local
                  protocol: TCP
                - containerPort: 10055
                  name: metrics
                  protocol: TCP
                volumeMounts:
                - name: kube-dns-config
                  mountPath: /kube-dns-config
              - name: dnsmasq
                image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4
                livenessProbe:
                  httpGet:
                    path: /healthcheck/dnsmasq
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                args:
                - -v=2
                - -logtostderr
                - -configDir=/etc/k8s/dns/dnsmasq-nanny
                - -restartDnsmasq=true
                - --
                - -k
                - --cache-size=1000
                - --log-facility=-
                - --server=/cluster.local/127.0.0.1#10053
                - --server=/in-addr.arpa/127.0.0.1#10053
                - --server=/ip6.arpa/127.0.0.1#10053
                ports:
                - containerPort: 53
                  name: dns
                  protocol: UDP
                - containerPort: 53
                  name: dns-tcp
                  protocol: TCP
                # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
                resources:
                  requests:
                    cpu: 150m
                    memory: 20Mi
                volumeMounts:
                - name: kube-dns-config
                  mountPath: /etc/k8s/dns/dnsmasq-nanny
              - name: sidecar
                image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4
                livenessProbe:
                  httpGet:
                    path: /metrics
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                args:
                - --v=2
                - --logtostderr
                - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
                - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
                ports:
                - containerPort: 10054
                  name: metrics
                  protocol: TCP
                resources:
                  requests:
                    memory: 20Mi
                    cpu: 10m
              dnsPolicy: Default
              serviceAccountName: kube-dns

  - path: /srv/kubernetes/manifests/kube-dns-svc.yaml
    content: |
        apiVersion: v1
        kind: Service
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            k8s-app: kube-dns
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: "KubeDNS"
        spec:
          selector:
            k8s-app: kube-dns
          clusterIP: 10.3.0.10
          ports:
          - name: dns
            port: 53
            protocol: UDP
          - name: dns-tcp
            port: 53
            protocol: TCP

  - path: /srv/kubernetes/manifests/heapster-sa.yaml
    content: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: heapster
          namespace: kube-system
          labels:
            kubernetes.io/cluster-service: "true"

  - path: /srv/kubernetes/manifests/heapster-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: heapster
          namespace: kube-system
          labels:
            k8s-app: heapster
            kubernetes.io/cluster-service: "true"
            version: v1.4.1
        spec:
          replicas: 1
          selector:
            matchLabels:
              k8s-app: heapster
              version: v1.4.1
          template:
            metadata:
              labels:
                k8s-app: heapster
                version: v1.4.1
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              serviceAccountName: heapster
              containers:
                - image: gcr.io/google_containers/heapster:v1.4.1
                  name: heapster
                  livenessProbe:
                    httpGet:
                      path: /healthz
                      port: 8082
                      scheme: HTTP
                    initialDelaySeconds: 180
                    timeoutSeconds: 5
                  resources:
                    limits:
                      cpu: 80m
                      memory: 200Mi
                    requests:
                      cpu: 80m
                      memory: 200Mi
                  command:
                    - /heapster
                    - --source=kubernetes.summary_api:''
                - image: gcr.io/google_containers/addon-resizer:2.0
                  name: heapster-nanny
                  resources:
                    limits:
                      cpu: 50m
                      memory: 90Mi
                    requests:
                      cpu: 50m
                      memory: 90Mi
                  env:
                    - name: MY_POD_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                    - name: MY_POD_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                  command:
                    - /pod_nanny
                    - --cpu=80m
                    - --extra-cpu=4m
                    - --memory=200Mi
                    - --extra-memory=4Mi
                    - --deployment=heapster
                    - --container=heapster
                    - --poll-period=300000

  

  - path: /srv/kubernetes/manifests/heapster-svc.yaml
    content: |
        kind: Service
        apiVersion: v1
        metadata:
          name: heapster
          namespace: kube-system
          labels:
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: "Heapster"
            k8s-app: heapster
        spec:
          ports:
            - port: 80
              targetPort: 8082
          selector:
            k8s-app: heapster

  - path: /srv/kubernetes/manifests/kube-dashboard-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: kubernetes-dashboard
          namespace: kube-system
          labels:
            k8s-app: kubernetes-dashboard
            version: v1.6.3
            kubernetes.io/cluster-service: "true"
        spec:
          replicas: 1
          selector:
            matchLabels:
              k8s-app: kubernetes-dashboard
          template:
            metadata:
              labels:
                k8s-app: kubernetes-dashboard
                version: v1.6.3
                kubernetes.io/cluster-service: "true"
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              containers:
              - name: kubernetes-dashboard
                image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.3
                resources:
                  limits:
                    cpu: 100m
                    memory: 50Mi
                  requests:
                    cpu: 100m
                    memory: 50Mi
                ports:
                - containerPort: 9090
                livenessProbe:
                  httpGet:
                    path: /
                    port: 9090
                  initialDelaySeconds: 30
                  timeoutSeconds: 30

  - path: /srv/kubernetes/manifests/kube-dashboard-svc.yaml
    content: |
        apiVersion: v1
        kind: Service
        metadata:
          name: kubernetes-dashboard
          namespace: kube-system
          labels:
            k8s-app: kubernetes-dashboard
            kubernetes.io/cluster-service: "true"
        spec:
          selector:
            k8s-app: kubernetes-dashboard
          ports:
          - port: 80
            targetPort: 9090

  - path: /srv/kubernetes/manifests/tiller.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          creationTimestamp: null
          labels:
            app: helm
            name: tiller
          name: tiller-deploy
          namespace: kube-system
        spec:
          strategy: {}
          template:
            metadata:
              creationTimestamp: null
              labels:
                app: helm
                name: tiller
              # Addition to the default tiller deployment for prioritizing tiller over other non-critical pods with rescheduler
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              tolerations:
              # Additions to the default tiller deployment for allowing to schedule tiller onto controller nodes
              # so that helm can be used to install pods running only on controller nodes
              - key: "node.alpha.kubernetes.io/role"
                operator: "Equal"
                value: "master"
                effect: "NoSchedule"
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              containers:
              - env:
                - name: TILLER_NAMESPACE
                  value: kube-system
                image: gcr.io/kubernetes-helm/tiller:v2.6.0
                imagePullPolicy: IfNotPresent
                livenessProbe:
                  httpGet:
                    path: /liveness
                    port: 44135
                  initialDelaySeconds: 1
                  timeoutSeconds: 1
                name: tiller
                ports:
                - containerPort: 44134
                  name: tiller
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 44135
                  initialDelaySeconds: 1
                  timeoutSeconds: 1
                resources: {}
              nodeSelector:
                beta.kubernetes.io/os: linux
        status: {}
        ---
        apiVersion: v1
        kind: Service
        metadata:
          creationTimestamp: null
          labels:
            app: helm
            name: tiller
          name: tiller-deploy
          namespace: kube-system
        spec:
          ports:
          - name: tiller
            port: 44134
            targetPort: tiller
          selector:
            app: helm
            name: tiller
          type: ClusterIP
        status:
          loadBalancer: {}

  - path: /srv/kube-aws/plugins/kubernetes-manifests
    encoding: gzip+base64
    content: H4sIAAAAAAAA/wEAAP//AAAAAAAAAAA=



  - path: /srv/kube-aws/plugins/helm-releases
    encoding: gzip+base64
    content: H4sIAAAAAAAA/wEAAP//AAAAAAAAAAA=





  - path: /etc/kubernetes/auth/kubelet-tls-bootstrap-token.tmp.enc
    encoding: gzip+base64
    content: H4sIAAAAAAAA/wDpARb+AQECAHgtAXNFpLmjNXHaaI4vnp9Kiu39c7e3/npF4nDh5ZaiJwAAAcAwggG8BgkqhkiG9w0BBwagggGtMIIBqQIBADCCAaIGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMEZyeUScvvR9bSzKsAgEQgIIBc0qY1qmLAKfbfdxyYaveBjlJiSXUQS6burs8BvXX79vvKBJULgXhmrj0tEDiqaOaD6BS6VnMSvif7MSLBPvIqyO+D5dUchPfrgOGire+25WpZ4H8B/SKt0esMDsHeyLLHAF0St75vZVgYU4D9WQbru0CuAozqQfUnilRgt9sGjK9SPUwWpsjf0ov5ebyBHJyPl2Y9vNjx7lm9lvu+l265+KuPiUQ/SAkETWExjvdE+YIXaenMMbX4H0dE5GBuTXeymx55YPcaivePp77jdZ9qhT4eNGeEkuMetjafe+ng5hp9skgRUtTa6FuGeLkgDu1gx2qFwiiYHa3crHXdU+mAqxQWdz7z1N/Y23X25JotYtRA0hxpMVMMDhUDh88rlCwKe/5srPpOj/l7LvbmoQZqx/WkMAqcuZDjmSggZ/tdkBKi+jT/h1Gll393OqsPUk/K8WwNKXItCEWYGfN69EuSXzK92qI48RNMK8lC0e1bk/jJjpUAQAA//9D+Scb6QEAAA==





  - path: /etc/kubernetes/ssl/ca.pem
    encoding: gzip+base64
    content: H4sIAAAAAAAA/2SUyc6rxhaF5zzFnVtXwG9MMziDKigaQwGF6Wc22PRgGzDN00fHURIpqeGnktaWvrX3/38/iDTD/p+MPN9QDRn46EspbBgyr8gyyOUCLAYEhQGBAmxYNK+yqTRpYSAgowoU0GIPLcqSKCEhJlrKdx6dW8qK1GO2I4IBpwE2QGDF+vUYVumFrRMf3NWFWe0dnLCPNqxgDu/pVV2Y7cuUZqP+glgOVuQDFxZ2CMHgy9rEJl0wJXG+Yw8sSvFNVtCSf5OtyC6xYUDKqP89LlIBcGRARPD7g1yYMiAIzGXMTdIgj6nzmVnhgxKuLpLRoe5zIIg3r2rGuGEey8Ktzp2rt/69g909bBmsBP5y45XX6dIxsyUF3q29KOvuMut2ds+Yog8kXAmz3bJTyOnFPWRxSStdVdiNBMHcWPeQqY709fZeieVf1cJ8BiPDGxo9KJ9XzlCLeL06T8fZWYmea0X0juwpHcTQ53TkapsAOs5rE92oNu+xj1emli0/La+Jq8W9dUAZtRVNKOEfnKLIzqowlHu5rE5OrTnV+XmA8YlZa2PkJlFiTcPXNbSU+9NR7iS/BT8fWqCesKXZoR/WNu1KeeffJznxpIig4/kQrTBHdKy5XgZvhpCbvN7tTJmxbtmLn8tHIIZCAQLgsOHF8IHzW6FORAgeIoIAy+BF/iyOx/iA6DQEwQIWBOn9H3PU3037qkNA55xMzwM67tLyJOVkPR9LiHqYRfwx3Tu+qPTwPsaNEklbIzsDxSV4aoj5QuuR6Sfs+SqY3aA3Do9D4g+hPZhZHbSu5k9l+Eme7DQnXZgKHs8+Db2qWmr/wbvaJm33eJcClynDspPIvbnRkQ0Z18VenIWvNDncgcr8HAXVGevLwA+9/QgWwPkslVuu6ZJCv3dpJHCxDtaybKU+4zoR0YzF8IvYqwSWmiVIdXM4v9t0/ZlO8ltHEveynxQtTTPt+/zGny46+xHfFn4nhzenb36sHdM9QOK2t5eUTSy52LM6Gj27YssajbaR65+Ooqe4NMkcurrISeGjQNFiPsqwQt0cC3sOxNnskuLXL+q7/chW/nsR/ggAAP//YTfvEi4EAAA=



  - path: /etc/kubernetes/ssl/apiserver.pem
    encoding: gzip+base64
    content: H4sIAAAAAAAA/2SUubKzShKEfZ5ifGICCYQW4ze6mwYaaBD74gmQ2EESSCxPP3HOjDH33vIqsyLSqC/y3z8DsULMfyHseEQmCHj4V2UoIVIvIQQGtwAzgaAgRGdrjtO1izE2s2THmj4kpPxmJrCxAW0wax52KDgrYO9jBpWU3ISgStx9mQt0wRKwYGEGEFAP8tM+7vwp5vGslplJvWy2PLzQGmyWRPmwzEyGes1sSWT3P1EIazBSB8/SHEuBbet4Lt95qLVGKAvZhiMK/d9ksFDrJ5j5Sc6UdksijU8iMhcFrijYKch9KS5JBcnGENg+AAcCpRn8+DoYCAQ2Or987cWcFfbLio/elfXr8+ttOmusbNVeUHU7LKfrVG/1/cqiMZMN7do433uL9TXhSYP3wjHlYyaAjvIOvvnjwsnt52aEZzs7bgo8+XfNfezIaXw+9+7xcK+3r3w8izSdIfGQEpm1hs6LzBTjreTSYI36h2bGrvY4b+gLbBdl2debV2wje5lemik7GuqkUWV3ccv6uewofF/e/ZJJOV71HN5LFyoe4gOusCeZ+OgTp1k8py/y1wB0eTht3MVSjKf0cRBtW23nd8Ll+GRTJlTluWjS4yQ4j0uVCPH+Wp6Qo5/o28mXlfvk5sSf5qv6vEXiRZDBzRR8VXqk8lfutP2TOfUBpLr09pp1mYRZBQWFACh1kRVzkfmzVMRS4OyuwFY5CGwJFGFBIf15Za7ZNmUoGBSERgXYvgxnCilR8h+QVAfjIvHnItER6n6ACta0C3ZJZCH8fzuTRPSTKEEXR8GYExL+UhlpnyRy2mwVmyRMyjxcdkZv8vGBNH/3mb8crGKdqsGWK8FqdMs35uWxKIP/ktiZQxKa76wrFyN02rwSP0nkIkYOf3n/xHzZxvy03lQ8JUrAG53Y5iTDqKAAeMCERfMqm0q5zDsI7FEGwELAxoAJvWnnnNbc2NLMeVjGxLZlW6cWlxurfLXS8rm8LbbLItjydXCWp/B54Gyx1dB4LX3RZOZ2J+6BoTw4YpwhsAudy80N6sP9/l2b+yHtjb4uS2muwpO/Kq+VACEAEoqlfnoGY8iIa14K/LF6NlWdVaZSwosbvjHlhOjx2sPiFfqOcsxpr6i5s4W6dUTWDpzXaBiOuloazPI9znbcFVzyuvjp7eWK0dKjkyeeTkvGJlc2/XhTBTwD9ik/r+tbH7ebOcjdPjfd/U5j6Ge8SHhcdS2vH3zOVzR7NxXUy0yW1VLJDEGgH9a92T2Mez83pddsJ2t5Dl5DjeShZZbL17jVEjtqZMjqQnruJXUFf/4wv42HTemfLfifAAAA///uyy62IgUAAA==

  - path: /etc/kubernetes/ssl/apiserver-key.pem.enc
    encoding: gzip+base64
    content: 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

  - path: /etc/kubernetes/ssl/etcd-client.pem
    encoding: gzip+base64
    content: H4sIAAAAAAAA/2SUybKySBCF9zxF740OlMFhWROTFFAyCTsEboGKAgKlPn3H/aNX3bk8ufgy40R8f/8OJKbt/YXIKbING4GI/EklattIxgiBOuRA2BBw26YHqwR2j0jOBGaZc3zmdrOUHmDEhQwIJyInCvYm2MREQg21CzVp83DTVCp9Ewx8yL0EAhpBZdpkXTxlChFWU3o0KoUfkTe9gq+PqZI2pSfR6CZ8bK//DdX0CmZ6IgKLDCeMHYloxip17m5qqOWX5BSWf8jgTdkvWMrDzb0yvZubeq8iTebq953Q/u/pEDKAOScB+N2zJ+KcQOA1T2mEvqOmsuvQcTcOpyFkp2b9Qs/JCuyp7NfOVmdNbhXGfJHvMsTdIEfXbGkXtlq3N62WjE/ZH2HA+UHVND2BnsWtxB9K1eJjcgfg3e7neF3N6+78jlfbH2+Rs/kYBVqqdIQstXQ9It34WkgAvA+9TUmOfXe3IuGrM1G6xv9oUaeOtVkdt2r7UNX5leWjC2HuXYLdAjypo/Jx88RdYO4f3uf7s9NDfOoBd392gUzbFCaxgr53cJbRwV5l1vbhlLW3xK4M6suErxK0bTIx49IaeNhyMObMeihudBomb4bzrv0EXHv5jtMMJ3Q4bsFby89pHH9CZ94nVb2TNNB3vkjuh02wyQv7Q5s1AoIAUNAHRbHAPMPJaR0AZskQMAx4yimkv1VWDmNUouBpIvQyAYsNKCiiYG2icDBD+6JiRiASMQCaDQFDkeXc0h1O66p8yMmgzsus6VIY9tMuiQ8xV0Bz077NS1k5cv5CL6A+brfO1S3ty68+bOX0+/V5KVw/PHTiDFbabt8DyXAKJ68beQZ+QV47rqd1dUNm1HhVncIKYCV/Nlz0c7XnPwOsp2oTbPPNTyKeavA9O9J1SXVtzQy3xQs2q9FWx8jB34v6dIs3FsakN6UeXpvWPbgDOP58fLEBie/tzYvYXZVBgnmdIv1WqGQd9v3KWvQMu0fnaZ4fvahf50ZniYYLb+rv4szq6LZ8lKzdPujdOvGnm0rWuYSoVoyxyj51Ygd4tc2LfiCXaCireUuz7j236ag71m2+FLvp/hhP2Tm8fHqD1I4ZSH+sQDz8f1P8EwAA///4XGfFRgQAAA==

  - path: /etc/kubernetes/ssl/etcd-client-key.pem.enc
    encoding: gzip+base64
    content: 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

  - path: /etc/kubernetes/ssl/etcd-trusted-ca.pem
    encoding: gzip+base64
    content: H4sIAAAAAAAA/2SUyc6rxhaF5zzFnVtXwG9MMziDKigaQwGF6Wc22PRgGzDN00fHURIpqeGnktaWvrX3/38/iDTD/p+MPN9QDRn46EspbBgyr8gyyOUCLAYEhQGBAmxYNK+yqTRpYSAgowoU0GIPLcqSKCEhJlrKdx6dW8qK1GO2I4IBpwE2QGDF+vUYVumFrRMf3NWFWe0dnLCPNqxgDu/pVV2Y7cuUZqP+glgOVuQDFxZ2CMHgy9rEJl0wJXG+Yw8sSvFNVtCSf5OtyC6xYUDKqP89LlIBcGRARPD7g1yYMiAIzGXMTdIgj6nzmVnhgxKuLpLRoe5zIIg3r2rGuGEey8Ktzp2rt/69g909bBmsBP5y45XX6dIxsyUF3q29KOvuMut2ds+Yog8kXAmz3bJTyOnFPWRxSStdVdiNBMHcWPeQqY709fZeieVf1cJ8BiPDGxo9KJ9XzlCLeL06T8fZWYmea0X0juwpHcTQ53TkapsAOs5rE92oNu+xj1emli0/La+Jq8W9dUAZtRVNKOEfnKLIzqowlHu5rE5OrTnV+XmA8YlZa2PkJlFiTcPXNbSU+9NR7iS/BT8fWqCesKXZoR/WNu1KeeffJznxpIig4/kQrTBHdKy5XgZvhpCbvN7tTJmxbtmLn8tHIIZCAQLgsOHF8IHzW6FORAgeIoIAy+BF/iyOx/iA6DQEwQIWBOn9H3PU3037qkNA55xMzwM67tLyJOVkPR9LiHqYRfwx3Tu+qPTwPsaNEklbIzsDxSV4aoj5QuuR6Sfs+SqY3aA3Do9D4g+hPZhZHbSu5k9l+Eme7DQnXZgKHs8+Db2qWmr/wbvaJm33eJcClynDspPIvbnRkQ0Z18VenIWvNDncgcr8HAXVGevLwA+9/QgWwPkslVuu6ZJCv3dpJHCxDtaybKU+4zoR0YzF8IvYqwSWmiVIdXM4v9t0/ZlO8ltHEveynxQtTTPt+/zGny46+xHfFn4nhzenb36sHdM9QOK2t5eUTSy52LM6Gj27YssajbaR65+Ooqe4NMkcurrISeGjQNFiPsqwQt0cC3sOxNnskuLXL+q7/chW/nsR/ggAAP//YTfvEi4EAAA=


  - path: /etc/kubernetes/controller-kubeconfig.yaml
    content: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            server: http://localhost:8080
        users:
        - name: kubelet
        contexts:
        - context:
            cluster: local
            user: kubelet
          name: kubelet-context
        current-context: kubelet-context



  - path: /etc/kubernetes/cni/net.d/10-calico.conf
    content: |
      {
        "name": "calico",
        "type": "flannel",
        "delegate": {
          "type": "calico",
          "etcd_endpoints": "#ETCD_ENDPOINTS#",
          "etcd_key_file": "/etc/kubernetes/ssl/etcd-client-key.pem",
          "etcd_cert_file": "/etc/kubernetes/ssl/etcd-client.pem",
          "etcd_ca_cert_file": "/etc/kubernetes/ssl/etcd-trusted-ca.pem",
          "log_level": "info",
          "policy": {
            "type": "k8s",
            "k8s_api_root": "http://127.0.0.1:8080/api/v1/"
          }
        }
      }







  - path: /srv/kubernetes/manifests/kube2iam-ds.yaml
    content: |
      apiVersion: extensions/v1beta1
      kind: DaemonSet
      metadata:
        name: kube2iam
        namespace: kube-system
        labels:
          app: kube2iam
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ''
      spec:
        updateStrategy:
          type: RollingUpdate
        template:
          metadata:
            labels:
              name: kube2iam
          spec:
            serviceAccountName: kube2iam
            hostNetwork: true
            tolerations:
            - operator: Exists
              effect: NoSchedule
            - operator: Exists
              effect: NoExecute
            - operator: Exists
              key: CriticalAddonsOnly
            containers:
              - image: jtblin/kube2iam:0.7.0
                name: kube2iam
                args:
                  - "--app-port=8282"
                  - "--auto-discover-base-arn"
                  - "--auto-discover-default-role"
                  - "--iptables=true"
                  - "--host-ip=$(HOST_IP)"
                  - "--host-interface=cali+"
           
                env:
                  - name: HOST_IP
                    valueFrom:
                      fieldRef:
                        fieldPath: status.podIP
                ports:
                  - containerPort: 8282
                    hostPort: 8282
                    name: http
                resources:
                  limits:
                    cpu: 10m
                    memory: 32Mi
                  requests:
                    cpu: 10m
                    memory: 32Mi
                securityContext:
                  privileged: true
  - path: /srv/kubernetes/manifests/kube2iam-rbac.yaml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: kube2iam
        namespace: kube-system
      ---

      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: ClusterRole
      metadata:
        annotations:
          rbac.authorization.kubernetes.io/autoupdate: "true"
        labels:
          kubernetes.io/bootstrapping: kube2iam
        name: kube2iam
      rules:
      - apiGroups:
        - ""
        resources:
        - pods
        - namespaces
        verbs:
        - get
        - list
        - watch
      ---

      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: ClusterRoleBinding
      metadata:
        name: kube2iam
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: kube2iam
      subjects:
      - kind: ServiceAccount
        name: kube2iam
        namespace: kube-system


  - path: /opt/bin/retry
    owner: root:root
    permissions: 0755
    content: |
      #!/bin/bash
      max_attempts="$1"; shift
      cmd="$@"
      attempt_num=1
      attempt_interval_sec=3

      until $cmd
      do
          if (( attempt_num == max_attempts ))
          then
              echo "Attempt $attempt_num failed and there are no more attempts left!"
              return 1
          else
              echo "Attempt $attempt_num failed! Trying again in $attempt_interval_sec seconds..."
              ((attempt_num++))
              sleep $attempt_interval_sec;
          fi
      done

