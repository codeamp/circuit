#cloud-config
coreos:
  update:
    reboot-strategy: "off"
  flannel:
    interface: $private_ipv4
    etcd_cafile: /etc/kubernetes/ssl/ca.pem
    etcd_certfile: /etc/kubernetes/ssl/etcd-client.pem
    etcd_keyfile: /etc/kubernetes/ssl/etcd-client-key.pem

  units:
    - name: telegraf.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Teleport Monitoring Agent
        After=network-online.target
        After=etcd-member.service
        [Service]
        Restart=on-failure
        ExecStart=/bin/rkt run \
          --set-env TELEGRAF_GLOBAL_TAGS_CLUSTER=production \
          --set-env TELEGRAF_GLOBAL_TAGS_SERVICE=controller \
          --set-env HOST_ETC=/etc2 \
          --interactive \
          --volume=varlibetcd2,kind=host,source=/var/lib/etcd2 \
          --volume=etc,kind=host,source=/etc \
          --volume=sys,kind=host,source=/sys \
          --volume=proc,kind=host,source=/proc \
          --mount volume=etc,target=/etc2 \
          --mount volume=proc,target=/proc \
          --mount volume=sys,target=/sys \
          --mount volume=varlibetcd2,target=/var/lib/etcd2 \
          --hostname=%H \
          --dns=host \
          --net=host \
          --insecure-options=image,ondisk \
          docker://checkr/telegraf-kapacitor:latest \
          --exec /usr/bin/telegraf -- --config /etc/generic.system.telegraf.conf
        
    - name: docker-healthcheck.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Run docker-healthcheck once
        After=docker.service
        
        [Service]
        Type=oneshot
        ExecStart=/opt/bin/docker-healthcheck
        
        [Install]
        WantedBy=multi-user.target
        
    - name: docker-healthcheck.timer
      command: start
      enable: true
      content: |
        [Unit]
        Description=Trigger docker-healthcheck periodically
        After=docker.service
        
        [Timer]
        OnUnitInactiveSec=30s
        Unit=docker-healthcheck.service
        
        [Install]
        WantedBy=multi-user.target
        
    - name: cfn-etcd-environment.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Fetches etcd static IP addresses list from CF
        After=network-online.target

        [Service]
        Restart=on-failure
        RemainAfterExit=true
        ExecStartPre=/opt/bin/cfn-etcd-environment
        ExecStart=/usr/bin/mv -f /var/run/coreos/etcd-environment /etc/etcd-environment




    - name: docker.service
      drop-ins:

        - name: 10-post-start-check.conf
          content: |
            [Service]
            RestartSec=10
            ExecStartPost=/usr/bin/docker pull gcr.io/google_containers/pause-amd64:3.0

        - name: 40-flannel.conf
          content: |
            [Unit]
            Wants=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
            ExecStartPre=/usr/bin/systemctl is-active flanneld.service

        - name: 60-logfilelimit.conf
          content: |
            [Service]
            Environment="DOCKER_OPTS=--log-opt max-size=50m --log-opt max-file=3"

    - name: flanneld.service
      drop-ins:
        - name: 10-etcd.conf
          content: |
            [Unit]
            Wants=cfn-etcd-environment.service
            After=cfn-etcd-environment.service

            [Service]
            EnvironmentFile=-/etc/etcd-environment
            Environment="ETCD_SSL_DIR=/etc/kubernetes/ssl"
            EnvironmentFile=-/run/flannel/etcd-endpoints.opts
            ExecStartPre=/usr/bin/systemctl is-active cfn-etcd-environment.service
            ExecStartPre=/bin/sh -ec "echo FLANNELD_ETCD_ENDPOINTS=${ETCD_ENDPOINTS} >/run/flannel/etcd-endpoints.opts"
            ExecStartPre=/opt/bin/decrypt-assets
            ExecStartPre=/usr/bin/etcdctl \
            --ca-file=/etc/kubernetes/ssl/ca.pem \
            --cert-file=/etc/kubernetes/ssl/etcd-client.pem \
            --key-file=/etc/kubernetes/ssl/etcd-client-key.pem \
            --endpoints="${ETCD_ENDPOINTS}" \
            set /coreos.com/network/config '{"Network" : "10.2.0.0/16", "Backend" : {"Type" : "vxlan"}}'
            TimeoutStartSec=120


    - name: kubelet.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=flanneld.service cfn-etcd-environment.service
        After=cfn-etcd-environment.service
        [Service]
        EnvironmentFile=-/etc/etcd-environment
        Environment=KUBELET_IMAGE_TAG=v1.6.3_coreos.0
        Environment=KUBELET_IMAGE_URL=quay.io/coreos/hyperkube
        Environment="RKT_RUN_ARGS=--volume dns,kind=host,source=/etc/resolv.conf \
        --set-env=ETCD_CA_CERT_FILE=/etc/kubernetes/ssl/ca.pem \
        --set-env=ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd-client.pem \
        --set-env=ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd-client-key.pem \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume var-lib-cni,kind=host,source=/var/lib/cni \
        --mount volume=var-lib-cni,target=/var/lib/cni \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        --volume etc-kubernetes,kind=host,source=/etc/kubernetes \
        --mount volume=etc-kubernetes,target=/etc/kubernetes"
        ExecStartPre=/usr/bin/systemctl is-active flanneld.service
        ExecStartPre=/usr/bin/systemctl is-active cfn-etcd-environment.service
        ExecStartPre=/usr/bin/mkdir -p /var/lib/cni
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/usr/bin/etcdctl \
                       --ca-file /etc/kubernetes/ssl/ca.pem \
                       --key-file /etc/kubernetes/ssl/etcd-client-key.pem \
                       --cert-file /etc/kubernetes/ssl/etcd-client.pem \
                       --endpoints "${ETCD_ENDPOINTS}" \
                       cluster-health

        ExecStartPre=/bin/sh -ec "find /etc/kubernetes/manifests /srv/kubernetes/manifests  -maxdepth 1 -type f | xargs --no-run-if-empty sed -i 's|#ETCD_ENDPOINTS#|${ETCD_ENDPOINTS}|'"
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --kubeconfig=/etc/kubernetes/controller-kubeconfig.yaml \
        --require-kubeconfig \
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
        --cni-bin-dir=/opt/cni/bin \
        --network-plugin=cni \
        --container-runtime=docker \
        --rkt-path=/usr/bin/rkt \
        --rkt-stage1-image=coreos.com/rkt/stage1-coreos \
        --register-with-taints=node.alpha.kubernetes.io/role=master:NoSchedule \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --cluster_dns=10.3.0.10 \
        --cluster_domain=cluster.local \
        --cloud-provider=aws
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target



    - name: install-kube-system.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=kubelet.service docker.service

        [Service]
        Type=simple
        StartLimitInterval=0
        RestartSec=10
        Restart=on-failure
        ExecStartPre=/usr/bin/systemctl is-active kubelet.service
        ExecStartPre=/usr/bin/systemctl is-active docker.service
        ExecStartPre=/usr/bin/curl -s -f http://127.0.0.1:8080/version
        ExecStart=/opt/bin/install-kube-system



    - name: cfn-signal.service
      command: start
      content: |
        [Unit]
        Wants=kubelet.service docker.service
        After=kubelet.service

        [Service]
        Type=oneshot
        ExecStartPre=/usr/bin/bash -c "while sleep 1; do if /usr/bin/curl -s -m 20 -f  http://127.0.0.1:8080/healthz > /dev/null &&  /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10252/healthz > /dev/null && /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10251/healthz > /dev/null &&  /usr/bin/curl --insecure -s -m 20 -f  https://127.0.0.1:10250/healthz > /dev/null ; then break ; fi;  done"
        
        ExecStart=/opt/bin/cfn-signal









write_files:
  - path: /etc/rkt/auth.d/docker.json
    permissions: 0600
    encoding: gzip+base64
    content: H4sIAAAAAAAA/6pWUCrKLvHOzEtRslJQSslPzk4tciwtyVDSAUuEpRYVZ+bngeTKDMFiqemZxSVFmanFSlYK0TBupa6hHkSvXma+UqyOglJyUWpKal5JZmIOSGG1glJpcWoRyJjk/JTUtJz88oKi/BSQgQWJxcXl+UVg60NLjaKcSqOS3EJDjDN8080CTAtylBRqa7kAAQAA//+faTbrpgAAAA==
  - path: /opt/bin/docker-healthcheck
    permissions: 0700
    encoding: gzip+base64
    content: H4sIAAAAAAAA/7yRMW7rMBBEe55ivv4BpMCAGwNpc4TUFDmiFqFJY7lWktsHEiTETYo0aQeLh/fI///6UUo/+jY7JxNMrqx3w9OAWMMbFbeGZ/SRS1/uOV9gM4vjhxgGN4lzDHNFtx9PXjJjt48vskhJB+g0oDHUEhuS+kCMnKoSymZeTUrqXMvkDafhVy6PAspQF+rm8INjM8kZsb6XC0wlJeqD5W7TufbZjNdg+ZgQajEvhRr34wP86mXVx/m70CqSLDyoa8q6Hewt8/wHmeuHbC/7FQAA//+efA6i6QEAAA==

  - path: /etc/kubernetes/additional-configs/cloud.config
    owner: root:root
    permissions: 0644
    content: |
      [global]
      DisableSecurityGroupIngress = true



  - path: /opt/bin/cfn-signal
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false \
        --mount volume=awsenv,target=/var/run/coreos \
        --uuid-file-save=/var/run/coreos/cfn-signal.uuid \
        --net=host \
        --trust-keys-from-https \
        quay.io/coreos/awscli:master --exec=/bin/bash -- \
          -ec \
          'instance_id=$(curl http://169.254.169.254/latest/meta-data/instance-id)
           stack_name=$(
             aws ec2 describe-tags --region us-east-1 --filters \
               "Name=resource-id,Values=$instance_id" \
               "Name=key,Values=aws:cloudformation:stack-name" \
               --output json \
             | jq -r ".Tags[].Value"
           )
           cfn-signal -e 0 --region us-east-1 --resource Controllers --stack $stack_name
          '

      rkt rm --uuid-file=/var/run/coreos/cfn-signal.uuid || :

  - path: /opt/bin/cfn-etcd-environment
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false \
        --mount volume=awsenv,target=/var/run/coreos \
        --uuid-file-save=/var/run/coreos/cfn-etcd-environment.uuid \
        --net=host \
        --trust-keys-from-https \
        quay.io/coreos/awscli:master --exec=/bin/bash -- \
          -ec \
          'instance_id=$(curl http://169.254.169.254/latest/meta-data/instance-id)
           stack_name=$(
             aws ec2 describe-tags --region us-east-1 --filters \
               "Name=resource-id,Values=$instance_id" \
               "Name=key,Values=aws:cloudformation:stack-name" \
               --output json \
             | jq -r ".Tags[].Value"
           )
           cfn-init -v -c "etcd-client" --region us-east-1 --resource Controllers --stack $stack_name
          '

      rkt rm --uuid-file=/var/run/coreos/cfn-etcd-environment.uuid || :

  - path: /opt/bin/install-kube-system
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e

      kubectl() {
          /usr/bin/docker run --rm --net=host -v /srv/kubernetes:/srv/kubernetes quay.io/coreos/hyperkube:v1.6.3_coreos.0 /hyperkube kubectl "$@"
      }

      while ! kubectl get ns kube-system; do
        echo Waiting until kube-system created.
        sleep 3
      done

      mfdir=/srv/kubernetes/manifests

      

      # Configmaps
      kubectl apply -f "${mfdir}/kube-dns-cm.yaml"

      # Serviceaccounts
      kubectl apply -f "${mfdir}/kube-dns-sa.yaml"

      # Deployments
      for manifest in {kube-dns-de,kube-dns-autoscaler-de,heapster-de}.yaml; do
          kubectl apply -f "${mfdir}/$manifest"
      done

      # Replicationcontrollers
      kubectl apply -f "${mfdir}/kube-dashboard-rc.yaml"

      # Services
      for manifest in {kube-dns,heapster,kube-dashboard}-svc.yaml;do
          kubectl apply -f "${mfdir}/$manifest"
      done

      

      

  - path: /etc/kubernetes/cni/docker_opts_cni.env
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""

  - path: /opt/bin/host-rkt
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/sh
      # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
      # through this rkt wrapper. It essentially enters the host mount namespace
      # (which it is already in) only for the purpose of breaking out of the chroot
      # before calling rkt. It makes things like rkt gc work and avoids bind mounting
      # in certain rkt filesystem dependancies into the kubelet rootfs. This can
      # eventually be obviated when the write-api stuff gets upstream and rkt gc is
      # through the api-server. Related issue:
      # https://github.com/coreos/rkt/issues/2878
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "$@"





  - path: /opt/bin/decrypt-assets
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=kube,kind=host,source=/etc/kubernetes,readOnly=false \
        --mount=volume=kube,target=/etc/kubernetes \
        --uuid-file-save=/var/run/coreos/decrypt-assets.uuid \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        --trust-keys-from-https \
        quay.io/coreos/awscli:master --exec=/bin/bash -- \
          -ec \
          'echo decrypting assets
           shopt -s nullglob
           for encKey in /etc/kubernetes/{ssl,}/*.enc; do
             echo decrypting $encKey
             f=$(mktemp $encKey.XXXXXXXX)
             /usr/bin/aws \
               --region us-east-1 kms decrypt \
               --ciphertext-blob fileb://$encKey \
               --output text \
               --query Plaintext \
             | base64 -d > $f
             mv -f $f ${encKey%.enc}
           done;
           echo done.'

      rkt rm --uuid-file=/var/run/coreos/decrypt-assets.uuid || :




  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-proxy
          namespace: kube-system
          labels:
            k8s-app: kube-proxy
          annotations:
            rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly

        spec:
          hostNetwork: true
          containers:
          - name: kube-proxy
            image: quay.io/coreos/hyperkube:v1.6.3_coreos.0
            command:
            - /hyperkube
            - proxy
            - --master=http://127.0.0.1:8080
            securityContext:
              privileged: true
            volumeMounts:
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
            - mountPath: /var/run/dbus
              name: dbus
              readOnly: false
          volumes:
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
          - hostPath:
              path: /var/run/dbus
            name: dbus

  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
        labels:
          k8s-app: kube-apiserver
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:v1.6.3_coreos.0
          command:
          - /hyperkube
          - apiserver
          - --apiserver-count=2
          - --bind-address=0.0.0.0
          - --etcd-servers=#ETCD_ENDPOINTS#
          - --etcd-cafile=/etc/kubernetes/ssl/ca.pem
          - --etcd-certfile=/etc/kubernetes/ssl/etcd-client.pem
          - --etcd-keyfile=/etc/kubernetes/ssl/etcd-client-key.pem
          - --allow-privileged=true
          - --service-cluster-ip-range=10.3.0.0/24
          - --secure-port=443
          
          - --storage-backend=etcd3
          
          - --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP
          
          
          
          
          - --advertise-address=$private_ipv4
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
          - --anonymous-auth=false
          - --cert-dir=/etc/kubernetes/ssl
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1/networkpolicies=true,batch/v2alpha1
          - --cloud-provider=aws
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          
          
          
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        
        
        

  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
        labels:
          k8s-app: kube-controller-manager
      spec:
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:v1.6.3_coreos.0
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --cloud-provider=aws
          
          - --node-monitor-grace-period=300s
          
          
          - --cloud-config=/etc/kubernetes/additional-configs/cloud.config
          
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          
          - mountPath: /etc/kubernetes/additional-configs
            name: additional-configs
            readOnly: true
          
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        hostNetwork: true
        volumes:
        
        - hostPath:
            path: /etc/kubernetes/additional-configs
          name: additional-configs
        
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host

  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
        labels:
          k8s-app: kube-scheduler
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:v1.6.3_coreos.0
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15

  - path: /srv/kubernetes/manifests/kube-dns-sa.yaml
    content: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            kubernetes.io/cluster-service: "true"

  - path: /srv/kubernetes/manifests/kube-dns-cm.yaml
    content: |
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: kube-dns
          namespace: kube-system

  - path: /srv/kubernetes/manifests/kube-dns-autoscaler-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: kube-dns-autoscaler
          namespace: kube-system
          labels:
            k8s-app: kube-dns-autoscaler
            kubernetes.io/cluster-service: "true"
        spec:
          template:
            metadata:
              labels:
                k8s-app: kube-dns-autoscaler
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              containers:
              - name: autoscaler
                image: gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.1.1
                resources:
                    requests:
                        cpu: "20m"
                        memory: "10Mi"
                command:
                  - /cluster-proportional-autoscaler
                  - --namespace=kube-system
                  - --configmap=kube-dns-autoscaler
                  - --target=Deployment/kube-dns
                  - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"min":2}}
                  - --logtostderr=true
                  - --v=2

  - path: /srv/kubernetes/manifests/kube-dns-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            k8s-app: kube-dns
            kubernetes.io/cluster-service: "true"
        spec:
          # replicas: not specified here:
          # 1. In order to make Addon Manager do not reconcile this replicas parameter.
          # 2. Default is 1.
          # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
          strategy:
            rollingUpdate:
              maxSurge: 10%
              maxUnavailable: 0
          selector:
            matchLabels:
              k8s-app: kube-dns
          template:
            metadata:
              labels:
                k8s-app: kube-dns
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              volumes:
              - name: kube-dns-config
                configMap:
                  name: kube-dns
                  optional: true
              tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              containers:
              - name: kubedns
                image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
                resources:
                  limits:
                    memory: 170Mi
                  requests:
                    cpu: 100m
                    memory: 70Mi
                livenessProbe:
                  httpGet:
                    path: /healthcheck/kubedns
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 8081
                    scheme: HTTP
                  initialDelaySeconds: 3
                  timeoutSeconds: 5
                args:
                - --domain=cluster.local.
                - --dns-port=10053
                - --config-dir=/kube-dns-config
                # This should be set to v=2 only after the new image (cut from 1.5) has
                # been released, otherwise we will flood the logs.
                - --v=2
                env:
                - name: PROMETHEUS_PORT
                  value: "10055"
                ports:
                - containerPort: 10053
                  name: dns-local
                  protocol: UDP
                - containerPort: 10053
                  name: dns-tcp-local
                  protocol: TCP
                - containerPort: 10055
                  name: metrics
                  protocol: TCP
                volumeMounts:
                - name: kube-dns-config
                  mountPath: /kube-dns-config
              - name: dnsmasq
                image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
                livenessProbe:
                  httpGet:
                    path: /healthcheck/dnsmasq
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                args:
                - -v=2
                - -logtostderr
                - -configDir=/etc/k8s/dns/dnsmasq-nanny
                - -restartDnsmasq=true
                - --
                - -k
                - --cache-size=1000
                - --log-facility=-
                - --server=/cluster.local/127.0.0.1#10053
                - --server=/in-addr.arpa/127.0.0.1#10053
                - --server=/ip6.arpa/127.0.0.1#10053
                ports:
                - containerPort: 53
                  name: dns
                  protocol: UDP
                - containerPort: 53
                  name: dns-tcp
                  protocol: TCP
                # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
                resources:
                  requests:
                    cpu: 150m
                    memory: 20Mi
                volumeMounts:
                - name: kube-dns-config
                  mountPath: /etc/k8s/dns/dnsmasq-nanny
              - name: sidecar
                image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
                livenessProbe:
                  httpGet:
                    path: /metrics
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                args:
                - --v=2
                - --logtostderr
                - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
                - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
                ports:
                - containerPort: 10054
                  name: metrics
                  protocol: TCP
                resources:
                  requests:
                    memory: 20Mi
                    cpu: 10m
              dnsPolicy: Default
              serviceAccountName: kube-dns

  - path: /srv/kubernetes/manifests/kube-dns-svc.yaml
    content: |
        apiVersion: v1
        kind: Service
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            k8s-app: kube-dns
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: "KubeDNS"
        spec:
          selector:
            k8s-app: kube-dns
          clusterIP: 10.3.0.10
          ports:
          - name: dns
            port: 53
            protocol: UDP
          - name: dns-tcp
            port: 53
            protocol: TCP

  - path: /srv/kubernetes/manifests/heapster-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: heapster-v1.3.0
          namespace: kube-system
          labels:
            k8s-app: heapster
            kubernetes.io/cluster-service: "true"
            version: v1.3.0
        spec:
          replicas: 1
          selector:
            matchLabels:
              k8s-app: heapster
              version: v1.3.0
          template:
            metadata:
              labels:
                k8s-app: heapster
                version: v1.3.0
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              containers:
                - image: gcr.io/google_containers/heapster:v1.3.0
                  name: heapster
                  livenessProbe:
                    httpGet:
                      path: /healthz
                      port: 8082
                      scheme: HTTP
                    initialDelaySeconds: 180
                    timeoutSeconds: 5
                  resources:
                    limits:
                      cpu: 80m
                      memory: 200Mi
                    requests:
                      cpu: 80m
                      memory: 200Mi
                  command:
                    - /heapster
                    - --source=kubernetes.summary_api:''
                - image: gcr.io/google_containers/addon-resizer:1.7
                  name: heapster-nanny
                  resources:
                    limits:
                      cpu: 50m
                      memory: 90Mi
                    requests:
                      cpu: 50m
                      memory: 90Mi
                  env:
                    - name: MY_POD_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                    - name: MY_POD_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                  command:
                    - /pod_nanny
                    - --cpu=80m
                    - --extra-cpu=4m
                    - --memory=200Mi
                    - --extra-memory=4Mi
                    - --threshold=5
                    - --deployment=heapster-v1.3.0
                    - --container=heapster
                    - --poll-period=300000
                    - --estimator=exponential

  - path: /srv/kubernetes/manifests/heapster-svc.yaml
    content: |
        kind: Service
        apiVersion: v1
        metadata:
          name: heapster
          namespace: kube-system
          labels:
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: "Heapster"
            k8s-app: heapster
        spec:
          ports:
            - port: 80
              targetPort: 8082
          selector:
            k8s-app: heapster

  - path: /srv/kubernetes/manifests/kube-dashboard-rc.yaml
    content: |
        apiVersion: v1
        kind: ReplicationController
        metadata:
          name: kubernetes-dashboard
          namespace: kube-system
          labels:
            k8s-app: kubernetes-dashboard
            version: v1.6.0
            kubernetes.io/cluster-service: "true"
        spec:
          replicas: 1
          selector:
            k8s-app: kubernetes-dashboard
          template:
            metadata:
              labels:
                k8s-app: kubernetes-dashboard
                version: v1.6.0
                kubernetes.io/cluster-service: "true"
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
            spec:
              tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              containers:
              - name: kubernetes-dashboard
                image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.0
                resources:
                  limits:
                    cpu: 100m
                    memory: 50Mi
                  requests:
                    cpu: 100m
                    memory: 50Mi
                ports:
                - containerPort: 9090
                livenessProbe:
                  httpGet:
                    path: /
                    port: 9090
                  initialDelaySeconds: 30
                  timeoutSeconds: 30

  - path: /srv/kubernetes/manifests/kube-dashboard-svc.yaml
    content: |
        apiVersion: v1
        kind: Service
        metadata:
          name: kubernetes-dashboard
          namespace: kube-system
          labels:
            k8s-app: kubernetes-dashboard
            kubernetes.io/cluster-service: "true"
        spec:
          selector:
            k8s-app: kubernetes-dashboard
          ports:
          - port: 80
            targetPort: 9090




  - path: /etc/kubernetes/ssl/ca.pem.enc
    encoding: gzip+base64
    content: H4sIAAAAAAAA/wC/BED7AQECAHgtAXNFpLmjNXHaaI4vnp9Kiu39c7e3/npF4nDh5ZaiJwAABJYwggSSBgkqhkiG9w0BBwagggSDMIIEfwIBADCCBHgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMIavdC0gAhlrmBg9qAgEQgIIESel5KdSj6HqSutYosDVzrkNvjnvi2q2dDWDwq+ctxFrAfauDsHuI1CNPJGtnmPrpjDEPbOhF0Tr3a4j0vWAt1Yycamc+RocCDASS9KWh8gvDanF8Dll6n+CdzgKMxatvA5x7Z0JcHo+yh1vsYMLLQCm0UWiru8Tr6j18+ZbD7z6BtmshpFKwbosWZotuqSLTBhYfuFIORUwXphGuCyJatmK14SbKA/4vblculPFDwc6OnqwJMcxbOoUgc8dVl/M8XHTudopDWrDYXtrMHpjyuzDgYqx8/DImdV5Mer66ftflF0O7RbB3JRyTPWd9vMzrE5g2yPKVh1wTb/tV+anU74WEuex1eCPBpxWfJyzGT8uP8UPngxsOocuVfOqUwXpYiwLA5teAJHRu2RVUwMurWnzeCVGvj6O9OHD53MOh5mG3+mfv3ZLZkJ4hOa/Ld/O4gn4E7dSFZZ76QRTI5Y9gFVYUhm39cH71fSFuH3Yb0UzL7xkT8w+bnXMPlx2ujKTElDE3XxRfW4yY4CPOEqP9FWQzgJwQpAo0oKL/INDUsjj3SUeGknd46fyKpItUniGiOLzfjFf8IMD+fqXtg5o3YofSR1dxoC28pNEDYWnaheeRJHL7cMs7TP5pSRYxPXxMTsOUleBPfj31EDp4NIyG24AFEpkoFCQt6xL3WXuvEgvqoctoZqhlD9pd2viUGN38TfjjB2Eur20YQAoOX74juIK5nvS3Iuwbi+6bxv8AdqPQkUC/sA/7bxTlorjvMQSdLg2yB3+qn6CLDEqs0Sa0Y40pN0T9uJ4cpAIBgev37I7+ZCT3c7bm2ljsSEmzeziNnatq44MZ7VYEGAXlegLwgyRYKjm0dLqVVJMlU1FTrdeyIRHYrb3glCI1o6PCZbaVCpONzzZIqEkcgGJYxTgso4HLv0ty9O61ziEjo3XUf9hLXrVekvV5kNJi/b4fOm04kT7loyFV05eaDWFmKMV8wbUn8kH96OH0+/b5ErsQMI1t71z0sZjzQ9CjvOn3HlQE7q8lb1XL5iNi4dBSuJb9xPlI66+gRRbxBeTPx6CauW3jOfMZIUnpYugljz1BTwiRHNvf99Rn3Jd21YhgoJqgyaP7JSIuCmbwXdpPQxs+vd7/r3wLcjum7GbZL0rjIp5FJu2h3VKa0Z7kzaBT87Jj0jadz2WOcFb2vkrrYmXE5PGJvnzqyKEa0kPiuaCxsW6fj8+DOs6BhdBgAaZ+3cAhUE7Ns/vXcCxWOpTadVkFvjJsfwH6fTVGN4G1qhuAK6fwPUWtR4trZ2vtuAWt2oOWXSMyZx/hWR7Cagl4ZzCzE/8IDAqtViW2z4Vn0Cc6NYHkfsGhFa/rwikalTGnQHpwaqLsiqIhRuCGuBf1Wtr8SKPEuQM9x5thKxOmtYNcBhVtmnvmCY5cOZi3xS7FKis4YiLJeeZGvPk1AvXbtvEW/Q6TMUC+Q9PL6wHOAQAA//+JGEI6vwQAAA==



  - path: /etc/kubernetes/ssl/apiserver.pem.enc
    encoding: gzip+base64
    content: H4sIAAAAAAAA/wCzBUz6AQECAHgtAXNFpLmjNXHaaI4vnp9Kiu39c7e3/npF4nDh5ZaiJwAABYowggWGBgkqhkiG9w0BBwagggV3MIIFcwIBADCCBWwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMTp/yDzQeGSmLQmaaAgEQgIIFPavrgTcz26Hz7ysZK8ZzgJalbPmuWVBwYY15VeYPKkEaS908x4WwNfccDoN3uKGH+udqAmvkjFg8YmnnlHmQx6gvsaXaJ28GL7g96glLKqZdx4Np1AE8ieP+toazo2+Kvg9ZFkFp/d5evGdEXHGBMarkbsFZsm+duR930BFa4zXN1mKIJjWNzmRtUARvxPMJb4VxAGElq3c34fgmNgnlcbp3cmC3iGtZWlSee8Z6CAIN55apB3jF6DJJ8xZJZ9yfWA6HjqXweUBkZ0iY+55axgAwcjcZyF8uu2Da8Jl1fX+36Za4ZkP2Cc9cPQNIbmlkGVwWwvzIfOvk/B+lfpfi7HpjW3/Mh5jT0xBfNlB6Nn4CkuMql+OM4Tjom1ClVXce7vI6YivkxyfvnEegRDd0GjRf+iEaVR9h4lKFrPkYNgf8ynyNO6gFMFCzYsAZCyVdUKcCXmInpfbVbWuda8Rviywlw+kUIIJob2lNLLW3M/+500PbXp+IanlETUCZGL8b4lAxxhx1HWE9ofQFX2m5/3ZZNVNd3a1aU4g0ERsa6cDuItg6GmbFXj/jIYaviDi7tHV0g1KTUDPLLVlfmQdAv2IrL5mHScMvVu9xwWQy34NBguG3BKquKnQnRSur43XtIFRpvWREXrVPPhghZewFYdRewoAXkCQAPTOGB2uh1XR8NMb4TcyabVCy+pDnxOB2Se1dAeu7iPsU8dbmLRbLEhk4XI82l3PtNY2aQlr29gq2nX5VhwUJE5NHZjzYahLRPraFNDubi6jJd8eWf/2VXO37eRKrgR2B+Uj/XFNFO9EsHzgKbSYzhMluDlc7KIJsoCYkFDnW+qZXJ+2NV0gZoqQiWDYHY620wV7v7JX5D06Xt/NF9i+Qg1Fq7EYfqSB+5/kUG3KqCokln4sNHQcEGCUcKcCi5Xf5ECr1lQ3N2igwU3TrAr2T08cnl5YAqwr0o5GOvR3CIZNeO24rDMOa5YFGADyyojjpFh+KHJbAKQlQHJhtp/kXwnzzR9B/QRuCvmD0CYGaaGZSIg/QUW6a6qFsbwC0cHW7ELcC/jZPBrB/30X/0jOaQ7DFBQtsjasrJfBERFT6JsKjH+K15mXP7IGdEJpCJcBdqqSbxTkozooSbW02CA43HtfnGEO/YTfTXe9H+Y9gQM+QlxrfIlwDp01Clc2Rodz79NdVKnjdgiIycP1SufnyHemtu8sr3qgugr76ksg4zgF5UrGURrEhQfedmUwNi0dDd6ff6rUGHUwUHoQ47lKfzhruTxxx0HHdjOAODrkO6HRHhMngXK7rp1ZjBKJ+FBnLeZJmyLetjncNzH4x0yc0NprFAy9XH0cGUl+W4NrirRnj0UVmZyXoNtK1x7C+Xi1XegcTlYzaher95Xw8j5pdtM6VitzPgq2d68dBU6UsNEnfZfxOaYDvqopyl/j3z93uu/4V0pgWplT2/24A2r3hNGmUu1QRp+aocPq5lEm3KuNA8iRjOpyh6Bcsc063Pola8160C7GJ5Xi1pyLMtwC/mKfQ28HU7IX7Xc4xksp9cLIf0GdI4KWVWg8Czmw9U4hnTuIt+BE1H0/zUUAxX18TkVshCMo3TEWdZa9abqabg+d/DGB6OS/fgMtEoc1l6Yo89T5cztcyR2k+P/g6rQf/Ws4MFLm13+z3SB6+HgV3nIR4ergCFAff4He2+HhYeOiiNjRDVsrprsoCPZJ6wfTauAFLCWfXZu1vKSB6p4n4ijWTN7OZ+a7lzgcacMKGUb/vD6eQKHf8YlZjW++E4Ml0Z+sF5pXkugEAAP//xxujdLMFAAA=

  - path: /etc/kubernetes/ssl/apiserver-key.pem.enc
    encoding: gzip+base64
    content: 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

  - path: /etc/kubernetes/ssl/etcd-client.pem.enc
    encoding: gzip+base64
    content: H4sIAAAAAAAA/wDXBCj7AQECAHgtAXNFpLmjNXHaaI4vnp9Kiu39c7e3/npF4nDh5ZaiJwAABK4wggSqBgkqhkiG9w0BBwagggSbMIIElwIBADCCBJAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMqgGl0NQOq6JNbc53AgEQgIIEYc+xRpRXbldPjRfenrc5y15CnOSMqCD7D106gJjONXH4+veet6jcXHTI//6PQ7xw2D0LI/HzxnMZbk1EQwjlfGr+NHHkViuFLPkYtbZCp6S3nNzycn7DwnPe+K80mGQu7OF5lGia9pkQ+ufpxuk/RFJmDBOXCr2RwWLn26F6Y3Da1bmjUsSDDwYlyqPWF2fXV2BG5Ir7J+Dl9gSGg3VEGz5zyj4YPlWNcLiiVx8ZtcFBQMHk5blv4wEkZyFsHTugnFgJSMKVMGSNus+vzuw7IPOhNESKhdGtOe5Fc30qFWCtCxTScr+jfDBq3EzAfwg9xNG38NakLA5I9amm73t4Umk2V16dGcvNrgNoaV8WmGp9fstVluBObpLtLSltiVkod+DeVp72+umzquNFov387EyU3KOt1ggt6x8JEXrwXz/O7wv22Q1twgQHApsph0ikPo1OlkdOU0CpKAaF5fyrMI7WKrMJIzkmIFjd2aE/PFfvkZ16MxpXDCrQPrbvj4ndSdYdhYGS1ukHO00H31eiJbs3aFlq2bbBWqIf9svZ1OuzQGIiVW/aLz2QEOytMP3/+M3EX6NbvaH8KtDqUWNXLDk23Uyay6elKUpCtlrFJf6Aj5jIvin2rjIUJWKbpEqVIuuq1WWYxYSsIf+9yPyg1XFcSaQy4Wt4KqHU0pJ7hXOXCce5B/rLGYBpdbnioEInwsBNHdZmwutz6KTRGSEVrif/7/21gaK41PipMjyeu8i9rLdc8jMC254T4FNC4CyY55WpRUCBfEvtYgr9R21OfnAl4utshQgPRNSDRcACJGKcGrPm35GEjjjdUwX4QTRUXimhY4AAw0bK3CuOORO7T+Pt9y8I19dA7la11PZk2JOcx0/9cLktV12sYFAm8tnEgjUVamIiBmFdfeM9NN3Sx+uRZbyZv7gPTAB+bSA/XHIeSHXgK3SzwRhMnROFVujfIu9boJL+eQZBq9GHQKEG05kpQfNVte48ta8s/qB0tOHP8YYNg5ziVcnB2zq5yITUj0XdA8dxYrf9C3b/CtmBGJIxi05VYug7EzmLZsUzkviOBKF0o6PhJxlpik2lDJ9fXr69bifCUIq9sMVHLvQFFK4aGq7NSRpJJUNtrfFXh5hB82UWrrLo6Wafl71LS0bzMec7EFQ8Ira8bUuXIyhfQkHjqkFcXeOZlteBBg9PvV5QJ40sfy+OwPC6ClFU+OUcbbhld8Lt1N8UoY6u9FFZ2hZlXlaj5Kzj2t02NkYLdNXb1PrO3vQhbL+0mnkaocObQDvComQk0jXIrZ/1wNItWTwt+irFruQQjcEP8S/UV34dEvBQlYJD+z6gKdTm3SCMx7PiBo5gRqs/fFo9U9Oo3JfIXcEsjzKeF+mIthdYUE1FAsZQJ+4HuCANn6xWYItDfOM8NvBhzp5sKM7ZmAS4rd2sFUVfVrmmt+JdMEF1iMDPM7mMeu+s7bxQYBJfW9w9Ku8Gn4kVQo6TsZ4U9BYKPzg0AQAA//+sLaaz1wQAAA==

  - path: /etc/kubernetes/ssl/etcd-client-key.pem.enc
    encoding: gzip+base64
    content: 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




  - path: /etc/kubernetes/controller-kubeconfig.yaml
    content: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            server: http://localhost:8080
        users:
        - name: kubelet
        contexts:
        - context:
            cluster: local
            user: kubelet
          name: kubelet-context
        current-context: kubelet-context


  - path: /etc/kubernetes/cni/net.d/10-flannel.conf
    content: |
        {
            "name": "podnet",
            "type": "flannel",
            "delegate": {
                "isDefaultGateway": true
            }
        }







