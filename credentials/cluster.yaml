# Unique name of Kubernetes cluster. In order to deploy
# more than one cluster into the same AWS account, this
# name must not conflict with an existing cluster.
clusterName: development

# CoreOS release channel to use. Currently supported options: alpha, beta, stable
# See coreos.com/releases for more information
releaseChannel: alpha
# KUBE18: 1562.1.0 Alpha;  docker 17.09.0 - testing complete
amiId: ami-316aa34b

# TTL in seconds for the Route53 RecordSet created if createRecordSet is set to true.
recordSetTTL: 30

# Name of the SSH keypair already loaded into the AWS
# account being used to deploy this cluster.
keyName: checkr-devops

# Additional keys to preload on the coreos account (keep this to a minimum)
# sshAuthorizedKeys:
# - "ssh-rsa AAAAEXAMPLEKEYEXAMPLEKEYEXAMPLEKEYEXAMPLEKEYEXAMPLEKEYEXAMPLEKEYEXAMPLEKEYEXAMPLEKEYEXAMPLEKEYEXAMPLEKEY example@example.org"

# Region to provision Kubernetes cluster
region: us-east-1

# ARN of the KMS key used to encrypt TLS assets.
kmsKeyArn: "arn:aws:kms:us-east-1:927411751048:key/a9f9e689-e794-4b46-88a3-7642150a697e"

apiEndpoints:
- # The unique name of this API endpoint used to identify it inside CloudFormation stacks or
  # to be referenced from other parts of cluster.yaml
  name: default
  dnsName: kube.checkrhq-dev.net
  loadBalancer:
    private: true
    hostedZone:
      id: "Z29MODHVVGZ870"
    subnets:
      - name: PrivateSubnet1
      - name: PrivateSubnet2
      - name: PrivateSubnet3
      - name: PrivateSubnet4

controller:
  iam:
    role:
      name: "controllerMR"
  customSystemdUnits:
    - name: telegraf.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Teleport Monitoring Agent
        After=network-online.target
        After=etcd-member.service
        [Service]
        Restart=on-failure
        ExecStart=/bin/rkt run \
          --stage1-name=coreos.com/rkt/stage1-fly:1.26.0 \
          --set-env TELEGRAF_GLOBAL_TAGS_CLUSTER=development \
          --set-env TELEGRAF_GLOBAL_TAGS_SERVICE=controller \
          --set-env HOST_ETC=/etc2 \
          --interactive \
          --volume=varlibetcd2,kind=host,source=/var/lib/etcd2 \
          --volume=etc,kind=host,source=/etc \
          --mount volume=etc,target=/etc2 \
          --mount volume=varlibetcd2,target=/var/lib/etcd2 \
          --hostname=%H \
          --dns=host \
          --net=host \
          --insecure-options=image,ondisk \
          docker://checkr/checkr-telegraf-kapacitor:latest \
          --exec /usr/bin/telegraf -- --config /etc/generic.system.telegraf.conf
    - name: docker-healthcheck.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Run docker-healthcheck once
        After=docker.service

        [Service]
        Type=oneshot
        ExecStart=/opt/bin/docker-healthcheck

        [Install]
        WantedBy=multi-user.target
    - name: docker-healthcheck.timer
      command: start
      enable: true
      content: |
        [Unit]
        Description=Trigger docker-healthcheck periodically
        After=docker.service

        [Timer]
        OnUnitInactiveSec=30s
        Unit=docker-healthcheck.service

        [Install]
        WantedBy=multi-user.target
  customFiles:
    - path: "/etc/rkt/auth.d/docker.json"
      permissions: 0600
      content: |
        { "rktKind": "dockerAuth", "rktVersion": "v1", "registries": ["registry-1.docker.io"], "credentials": { "user": "codeflowprod", "password": "Uu2ZBuZbFUT3hMg6P5pl" }}
    - path: "/opt/bin/docker-healthcheck"
      owner: root:root
      permissions: 0700
      content: |
        #!/bin/bash

        if timeout 10 docker ps > /dev/null; then
        exit 0
        fi

        echo "docker failed"
        echo "Giving docker 30 seconds grace before restarting"
        sleep 30

        if timeout 10 docker ps > /dev/null; then
        echo "docker recovered"
        exit 0
        fi

        echo "docker still down; triggering docker restart"
        systemctl restart containerd docker

        echo "Waiting 60 seconds to give docker time to start"
        sleep 60

        if timeout 10 docker ps > /dev/null; then
        echo "docker recovered"
        exit 0
        fi

        echo "docker still failing"
  instanceType: r4.large
  rootVolume:
    size: 100
    type: gp2
  createTimeout: PT20M
  autoScalingGroup:
    minSize: 2
    maxSize: 4
    rollingUpdateMinInstancesInService: 2
  subnets:
    - name: PrivateSubnet1
    - name: PrivateSubnet2
    - name: PrivateSubnet3
    - name: PrivateSubnet4

worker:
  nodePools:
    - name: default
      iam:
        role:
          name: "workerMR"
      nodeStatusUpdateFrequency: "20s"
      awsNodeLabels:
        enabled: true
        #      clusterAutoscalerSupport:
        #enabled: true
      nodeDrainer:
        enabled: true
      nodeLabels:
        checkr/role: default
      autoScalingGroup:
        minSize: 1 
        maxSize: 4
        rollingUpdateMinInstancesInService: 2
      subnets:
        - name: PrivateSubnet1
        - name: PrivateSubnet2
        - name: PrivateSubnet3
        - name: PrivateSubnet4
      waitSignal:
        enabled: true
        maxBatchSize: 2
      createTimeout: PT20M
      instanceType: r4.large
      rootVolume:
        size: 200
        type: gp2
      securityGroupIds: ["sg-937ac5ec"]
      customSystemdUnits:
        - name: telegraf.service
          command: start
          enable: true
          content: |
            [Unit]
            Description=Teleport Monitoring Agent
            After=network-online.target
            After=etcd-member.service
            [Service]
            Restart=on-failure
            ExecStart=/bin/rkt run \
              --stage1-name=coreos.com/rkt/stage1-fly:1.26.0 \
              --set-env TELEGRAF_GLOBAL_TAGS_CLUSTER=development \
              --set-env TELEGRAF_GLOBAL_TAGS_SERVICE=worker \
              --set-env HOST_ETC=/etc2 \
              --interactive \
              --volume=varlibetcd2,kind=host,source=/var/lib/etcd2 \
              --volume=etc,kind=host,source=/etc \
              --mount volume=etc,target=/etc2 \
              --mount volume=varlibetcd2,target=/var/lib/etcd2 \
              --hostname=%H \
              --dns=host \
              --net=host \
              --insecure-options=image,ondisk \
              docker://checkr/checkr-telegraf-kapacitor:latest \
              --exec /usr/bin/telegraf -- --config /etc/generic.system.telegraf.conf
        - name: docker-healthcheck.service
          command: start
          enable: true
          content: |
            [Unit]
            Description=Run docker-healthcheck once
            After=docker.service

            [Service]
            Type=oneshot
            ExecStart=/opt/bin/docker-healthcheck

            [Install]
            WantedBy=multi-user.target
        - name: docker-healthcheck.timer
          command: start
          enable: true
          content: |
            [Unit]
            Description=Trigger docker-healthcheck periodically
            After=docker.service

            [Timer]
            OnUnitInactiveSec=30s
            Unit=docker-healthcheck.service

            [Install]
            WantedBy=multi-user.target
      customFiles:
        - path: "/etc/rkt/auth.d/docker.json"
          permissions: 0600
          content: |
            { "rktKind": "dockerAuth", "rktVersion": "v1", "registries": ["registry-1.docker.io"], "credentials": { "user": "codeflowprod", "password": "Uu2ZBuZbFUT3hMg6P5pl" }}
        - path: "/opt/bin/docker-healthcheck"
          owner: root:root
          permissions: 0700
          content: |
            #!/bin/bash
            
            if timeout 10 docker ps > /dev/null; then
            exit 0
            fi

            echo "docker failed"
            echo "Giving docker 30 seconds grace before restarting"
            sleep 30

            if timeout 10 docker ps > /dev/null; then
            echo "docker recovered"
            exit 0
            fi

            echo "docker still down; triggering docker restart"
            systemctl restart containerd docker

            echo "Waiting 60 seconds to give docker time to start"
            sleep 60

            if timeout 10 docker ps > /dev/null; then
            echo "docker recovered"
            exit 0
            fi

            echo "docker still failing"
etcd:
  customSystemdUnits:
    - name: telegraf.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Teleport Monitoring Agent
        After=network-online.target
        After=etcd-member.service
        [Service]
        Restart=on-failure
        ExecStart=/bin/rkt run \
          --stage1-name=coreos.com/rkt/stage1-fly:1.26.0 \
          --set-env TELEGRAF_GLOBAL_TAGS_CLUSTER=development \
          --set-env TELEGRAF_GLOBAL_TAGS_SERVICE=etcd \
          --set-env HOST_ETC=/etc2 \
          --interactive \
          --volume=varlibetcd2,kind=host,source=/var/lib/etcd2 \
          --volume=etc,kind=host,source=/etc \
          --mount volume=etc,target=/etc2 \
          --mount volume=varlibetcd2,target=/var/lib/etcd2 \
          --hostname=%H \
          --dns=host \
          --net=host \
          --insecure-options=image,ondisk \
          docker://checkr/checkr-telegraf-kapacitor:latest \
          --exec /usr/bin/telegraf -- --config /etc/generic.system.telegraf.conf
  subnets:
    - name: PrivateSubnet1
    - name: PrivateSubnet2
    - name: PrivateSubnet3
    - name: PrivateSubnet4
  count: 1
  version: 3.2.9
  rootVolume:
    size: 30
    type: gp2
  dataVolume:
    size: 100
    type: gp2
    encrypted: true
  instanceType: r4.large
  snapshot:
    automated: true
  disasterRecovery:
    automated: true
  customFiles:
    - path: "/etc/rkt/auth.d/docker.json"
      permissions: 0600
      content: |
        { "rktKind": "dockerAuth", "rktVersion": "v1", "registries": ["registry-1.docker.io"], "credentials": { "user": "codeflowprod", "password": "Uu2ZBuZbFUT3hMg6P5pl" }}

## Networking config

# ID of existing VPC to create subnet in. Leave blank to create a new VPC
vpc:
  id: vpc-453b9822

#vpcId: vpc-453b9822

#internetGatewayId: igw-c1713fa5

# ID of existing route table in existing VPC to attach subnet to. Leave blank to use the VPC's main route table.
# routeTableId:

# CIDR for Kubernetes VPC. If vpcId is specified, must match the CIDR of existing vpc.
vpcCIDR: "10.10.0.0/16"

# CIDR for Kubernetes subnet when placing nodes in a single availability zone (not highly-available) Leave commented out for multi availability zone setting and use the below `subnets` section instead.
# instanceCIDR: "10.0.0.0/24"

# Kubernetes subnets with their CIDRs and availability zones. Differentiating availability zone for 2 or more subnets result in high-availability (failures of a single availability zone won't result in immediate downtimes)
subnets:
  - name: PrivateSubnet1
    availabilityZone: us-east-1a
    instanceCIDR: "10.10.100.0/24"
    mapPublicIps: false
    id: "subnet-93095ccb"
    private: true
    routeTable:
      id: "rtb-5299b035"
  - name: PrivateSubnet2
    availabilityZone: us-east-1c
    instanceCIDR: "10.10.101.0/24"
    mapPublicIps: false
    id: "subnet-5bdb9271"
    private: true
    routeTable:
      id: "rtb-5599b032"
  - name: PrivateSubnet3
    availabilityZone: us-east-1d
    instanceCIDR: "10.10.102.0/24"
    mapPublicIps: false
    id: "subnet-af6b53d9"
    private: true
    routeTable:
      id: "rtb-5399b034"
  - name: PrivateSubnet4
    availabilityZone: us-east-1e
    instanceCIDR: "10.10.103.0/24"
    mapPublicIps: false
    id: "subnet-0117233c"
    private: true
    routeTable:
      id: "rtb-5499b033"

# CIDR for all service IP addresses
# serviceCIDR: "10.3.0.0/24"

# CIDR for all pod IP addresses
# podCIDR: "10.2.0.0/16"

# IP address of Kubernetes dns service (must be contained by serviceCIDR)
# dnsServiceIP: 10.3.0.10

# Expiration in days from creation time of TLS assets. By default, the CA will
# expire in 10 years and the server and client certificates will expire in 1
# year.
tlsCADurationDays: 3650
tlsCertDurationDays: 712

# Version of hyperkube image to use. This is the tag for the hyperkube image repository.
#kubernetesVersion: v1.8.0_coreos.0
kubernetesVersion: v1.8.1

hyperkubeImage:
 repo: gcr.io/google_containers/hyperkube
 rktPullDocker: true

# Create MountTargets for a pre-existing Elastic File System (Amazon EFS). Enter the resource id, eg "fs-47a2c22e"
# This is a NFS share that will be available across the entire cluster through a hostPath volume on the "/efs" mountpoint
#
# You can create a new EFS volume using the CLI:
# $ aws efs create-file-system --creation-token $(uuidgen)
#elasticFileSystemId: fs-47a2c22e

# Determines the container runtime for kubernetes to use. Accepts 'docker' or 'rkt'.
containerRuntime: docker

# Custom docker options
#dockerOpts: "--log-driver=journald"

# Experimental features will change in backward-incompatible ways
experimental:
  kube2IamSupport:
    enabled: true
  disableSecurityGroupIngress: true
  nodeMonitorGracePeriod: "300s"
  admission:
  #  podSecurityPolicy:
  #    enabled: true
    denyEscalatingExec:
      enabled: true
  auditLog:
    enabled: true
    maxage: 30
    logpath: /dev/stdout

flannelImage:
  repo: quay.io/coreos/flannel
  tag: v0.9.0
  rktPullDocker: false

useCalico: true
  
calicoNodeImage:
  repo: quay.io/calico/node
  tag: v2.6.1
  rktPullDocker: false

# Calico CNI image repository to use.
calicoCniImage:
  repo: quay.io/calico/cni
  tag: v1.11.0
  rktPullDocker: false

# Calico Kube Controllers image repository to use.
calicoKubeControllersImage:
  repo: quay.io/calico/kube-controllers
  tag: v1.0.0
  rktPullDocker: false

calicoCtlImage:
  repo: calico/ctl
  tag: v1.6.1
  rktPullDocker: false
